{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2431805,"sourceType":"datasetVersion","datasetId":8782}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report, roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport hashlib\nimport json\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\nRANDOM_SEED = 42\ntorch.manual_seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\n\nclass FlowerDataset(Dataset):\n    def __init__(self, image_paths, labels, transform=None):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        label = self.labels[idx]\n        \n        image = Image.open(img_path).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n            \n        return image, label\n\ndef load_flower_dataset(base_path):\n    classes = ['daisy', 'dandelion', 'rose', 'sunflower', 'tulip']\n    \n    all_image_paths = []\n    all_labels = []\n    class_counts = {}\n    \n    for class_idx, class_name in enumerate(classes):\n        class_path = os.path.join(base_path, class_name)\n        \n        if not os.path.exists(class_path):\n            print(f\"Warning: {class_path} does not exist!\")\n            continue\n            \n        images = [f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n        \n        class_counts[class_name] = len(images)\n        \n        for img_name in images:\n            img_path = os.path.join(class_path, img_name)\n            all_image_paths.append(img_path)\n            all_labels.append(class_idx)\n    \n    print(\"\\nDataset Distribution:\")\n    print(\"-\" * 40)\n    for class_name, count in class_counts.items():\n        print(f\"  {class_name:12s}: {count:4d} images\")\n    print(\"-\" * 40)\n    print(f\"  Total:        {len(all_image_paths):4d} images\")\n    print(\"-\" * 40)\n    \n    return all_image_paths, all_labels, classes\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        \n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n    \n    def forward(self, x):\n        identity = x\n        \n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        \n        out = self.conv2(out)\n        out = self.bn2(out)\n        \n        out += self.shortcut(identity)\n        out = self.relu(out)\n        \n        return out\n\nclass FlowerCNN(nn.Module):\n    def __init__(self, num_classes=5):\n        super(FlowerCNN, self).__init__()\n        \n        self.initial_conv = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        )\n        \n        self.layer1 = nn.Sequential(\n            ResidualBlock(64, 128, stride=1),\n            ResidualBlock(128, 128, stride=1)\n        )\n        \n        self.layer2 = nn.Sequential(\n            ResidualBlock(128, 256, stride=2),\n            ResidualBlock(256, 256, stride=1)\n        )\n        \n        self.layer3 = nn.Sequential(\n            ResidualBlock(256, 512, stride=2),\n            ResidualBlock(512, 512, stride=1)\n        )\n        \n        self.layer4 = nn.Sequential(\n            ResidualBlock(512, 1024, stride=2),\n            ResidualBlock(1024, 1024, stride=1)\n        )\n        \n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(1024, 512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            nn.ReLU(inplace=True),\n            nn.Linear(256, num_classes)\n        )\n        \n    def forward(self, x):\n        x = self.initial_conv(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\nclass FixedWeightWatermark:\n    def __init__(self, model, fingerprint, freeze_ratio=0.002):\n        self.model = model\n        self.fingerprint = fingerprint\n        self.freeze_ratio = freeze_ratio\n        self.frozen_params = {}\n        self.frozen_values = {}\n        \n    def generate_frozen_pattern(self):\n        hash_object = hashlib.sha256(self.fingerprint.encode())\n        hex_dig = hash_object.hexdigest()\n        seed = int(hex_dig[:8], 16)\n        rng = np.random.RandomState(seed)\n        \n        all_params = []\n        for name, param in self.model.named_parameters():\n            if 'weight' in name and ('layer' in name or 'classifier' in name or 'initial_conv' in name):\n                all_params.append((name, param))\n        \n        total_frozen = 0\n        for name, param in all_params:\n            num_weights = param.numel()\n            num_freeze = max(1, int(num_weights * self.freeze_ratio))\n            \n            indices = rng.choice(num_weights, size=num_freeze, replace=False)\n            values = rng.randn(num_freeze) * 0.1\n            \n            self.frozen_params[name] = indices\n            self.frozen_values[name] = values\n            total_frozen += num_freeze\n        \n        print(f\"\\nWatermark Configuration:\")\n        print(f\"  Fingerprint: {self.fingerprint}\")\n        print(f\"  Freeze Ratio: {self.freeze_ratio:.4f}\")\n        print(f\"  Total Frozen Weights: {total_frozen}\")\n        print(f\"  Frozen Layers: {len(self.frozen_params)}\")\n    \n    def apply_frozen_weights(self):\n        with torch.no_grad():\n            for name, param in self.model.named_parameters():\n                if name in self.frozen_params:\n                    indices = self.frozen_params[name]\n                    values = self.frozen_values[name]\n                    \n                    param_flat = param.data.view(-1)\n                    param_flat[indices] = torch.FloatTensor(values).to(param.device)\n    \n    def freeze_gradient_hook(self):\n        def hook_fn(name):\n            def hook(grad):\n                if name in self.frozen_params:\n                    indices = self.frozen_params[name]\n                    grad_flat = grad.view(-1)\n                    grad_flat[indices] = 0.0\n                return grad\n            return hook\n        \n        for name, param in self.model.named_parameters():\n            if name in self.frozen_params:\n                param.register_hook(hook_fn(name))\n    \n    def verify_frozen_weights(self):\n        match_score = 0.0\n        total_frozen = 0\n        layer_matches = {}\n        \n        with torch.no_grad():\n            for name, param in self.model.named_parameters():\n                if name in self.frozen_params:\n                    indices = self.frozen_params[name]\n                    expected_values = torch.FloatTensor(self.frozen_values[name]).to(param.device)\n                    \n                    param_flat = param.data.view(-1)\n                    actual_values = param_flat[indices]\n                    \n                    differences = torch.abs(actual_values - expected_values)\n                    matches = (differences < 0.01).sum().item()\n                    \n                    layer_matches[name] = matches / len(indices)\n                    match_score += matches\n                    total_frozen += len(indices)\n        \n        match_ratio = match_score / total_frozen if total_frozen > 0 else 0.0\n        \n        return match_ratio, total_frozen, layer_matches\n    \n    def get_watermark_statistics(self):\n        stats = {\n            'total_layers': len(self.frozen_params),\n            'total_frozen_weights': sum(len(indices) for indices in self.frozen_params.values()),\n            'layer_details': {}\n        }\n        \n        for name, indices in self.frozen_params.items():\n            stats['layer_details'][name] = {\n                'num_frozen': len(indices),\n                'frozen_values_mean': float(np.mean(self.frozen_values[name])),\n                'frozen_values_std': float(np.std(self.frozen_values[name]))\n            }\n        \n        return stats\n\ndef train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, \n                num_epochs, watermark_handler=None, model_name=\"model\"):\n    \n    history = {\n        'train_loss': [],\n        'val_loss': [],\n        'train_acc': [],\n        'val_acc': [],\n        'learning_rates': []\n    }\n    \n    best_val_acc = 0.0\n    \n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [{model_name}]')\n        for inputs, labels in pbar:\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            if watermark_handler is not None:\n                watermark_handler.apply_frozen_weights()\n            \n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n            \n            pbar.set_postfix({'loss': running_loss/len(pbar), 'acc': 100.*correct/total})\n        \n        train_loss = running_loss / len(train_loader)\n        train_acc = 100. * correct / total\n        \n        model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                \n                val_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n        \n        val_loss = val_loss / len(val_loader)\n        val_acc = 100. * correct / total\n        \n        current_lr = optimizer.param_groups[0]['lr']\n        \n        history['train_loss'].append(train_loss)\n        history['val_loss'].append(val_loss)\n        history['train_acc'].append(train_acc)\n        history['val_acc'].append(val_acc)\n        history['learning_rates'].append(current_lr)\n        \n        if scheduler is not None:\n            scheduler.step(val_loss)\n        \n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save(model.state_dict(), f'{model_name}_best.pth')\n        \n        print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, '\n              f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, LR: {current_lr:.6f}')\n    \n    return history\n\ndef evaluate_model(model, test_loader, class_names):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    all_probs = []\n    \n    with torch.no_grad():\n        for inputs, labels in tqdm(test_loader, desc='Evaluating'):\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            probs = torch.softmax(outputs, dim=1)\n            _, predicted = outputs.max(1)\n            \n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            all_probs.extend(probs.cpu().numpy())\n    \n    all_preds = np.array(all_preds)\n    all_labels = np.array(all_labels)\n    all_probs = np.array(all_probs)\n    \n    accuracy = accuracy_score(all_labels, all_preds)\n    precision, recall, f1, support = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n    \n    per_class_precision, per_class_recall, per_class_f1, per_class_support = precision_recall_fscore_support(\n        all_labels, all_preds, average=None\n    )\n    \n    cm = confusion_matrix(all_labels, all_preds)\n    report = classification_report(all_labels, all_preds, target_names=class_names, digits=4)\n    \n    metrics = {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1,\n        'per_class_precision': per_class_precision,\n        'per_class_recall': per_class_recall,\n        'per_class_f1': per_class_f1,\n        'per_class_support': per_class_support,\n        'confusion_matrix': cm,\n        'predictions': all_preds,\n        'labels': all_labels,\n        'probabilities': all_probs,\n        'classification_report': report\n    }\n    \n    return metrics\n\ndef plot_training_history(history, model_name):\n    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n    \n    epochs = range(1, len(history['train_loss']) + 1)\n    \n    axes[0].plot(epochs, history['train_loss'], label='Train Loss', marker='o', \n                 linewidth=2.5, markersize=6, color='#E74C3C')\n    axes[0].plot(epochs, history['val_loss'], label='Validation Loss', marker='s', \n                 linewidth=2.5, markersize=6, color='#3498DB')\n    axes[0].set_xlabel('Epoch', fontsize=13, fontweight='bold')\n    axes[0].set_ylabel('Loss', fontsize=13, fontweight='bold')\n    axes[0].set_title(f'Loss Curves - {model_name}', fontsize=15, fontweight='bold', pad=15)\n    axes[0].legend(fontsize=11, frameon=True, shadow=True)\n    axes[0].grid(True, alpha=0.3, linestyle='--')\n    \n    axes[1].plot(epochs, history['train_acc'], label='Train Accuracy', marker='o', \n                 linewidth=2.5, markersize=6, color='#2ECC71')\n    axes[1].plot(epochs, history['val_acc'], label='Validation Accuracy', marker='s', \n                 linewidth=2.5, markersize=6, color='#F39C12')\n    axes[1].set_xlabel('Epoch', fontsize=13, fontweight='bold')\n    axes[1].set_ylabel('Accuracy (%)', fontsize=13, fontweight='bold')\n    axes[1].set_title(f'Accuracy Curves - {model_name}', fontsize=15, fontweight='bold', pad=15)\n    axes[1].legend(fontsize=11, frameon=True, shadow=True)\n    axes[1].grid(True, alpha=0.3, linestyle='--')\n    \n    axes[2].plot(epochs, history['learning_rates'], marker='o', \n                 linewidth=2.5, markersize=6, color='#9B59B6')\n    axes[2].set_xlabel('Epoch', fontsize=13, fontweight='bold')\n    axes[2].set_ylabel('Learning Rate', fontsize=13, fontweight='bold')\n    axes[2].set_title(f'Learning Rate Schedule - {model_name}', fontsize=15, fontweight='bold', pad=15)\n    axes[2].grid(True, alpha=0.3, linestyle='--')\n    axes[2].set_yscale('log')\n    \n    plt.tight_layout()\n    plt.savefig(f'{model_name}_training_history.png', dpi=300, bbox_inches='tight', facecolor='white')\n    plt.close()\n\ndef plot_confusion_matrix(cm, class_names, model_name):\n    plt.figure(figsize=(10, 8))\n    \n    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    \n    sns.heatmap(cm, annot=True, fmt='d', cmap='YlOrRd', \n                xticklabels=class_names, yticklabels=class_names,\n                cbar_kws={'label': 'Count'}, linewidths=0.5, linecolor='gray')\n    \n    plt.xlabel('Predicted Label', fontsize=13, fontweight='bold')\n    plt.ylabel('True Label', fontsize=13, fontweight='bold')\n    plt.title(f'Confusion Matrix - {model_name}', fontsize=15, fontweight='bold', pad=15)\n    plt.xticks(rotation=45, ha='right', fontsize=11)\n    plt.yticks(rotation=0, fontsize=11)\n    plt.tight_layout()\n    plt.savefig(f'{model_name}_confusion_matrix.png', dpi=300, bbox_inches='tight', facecolor='white')\n    plt.close()\n    \n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues', \n                xticklabels=class_names, yticklabels=class_names,\n                cbar_kws={'label': 'Percentage'}, linewidths=0.5, linecolor='gray')\n    \n    plt.xlabel('Predicted Label', fontsize=13, fontweight='bold')\n    plt.ylabel('True Label', fontsize=13, fontweight='bold')\n    plt.title(f'Normalized Confusion Matrix - {model_name}', fontsize=15, fontweight='bold', pad=15)\n    plt.xticks(rotation=45, ha='right', fontsize=11)\n    plt.yticks(rotation=0, fontsize=11)\n    plt.tight_layout()\n    plt.savefig(f'{model_name}_confusion_matrix_normalized.png', dpi=300, bbox_inches='tight', facecolor='white')\n    plt.close()\n\ndef plot_per_class_metrics(metrics, class_names, model_name):\n    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n    \n    x = np.arange(len(class_names))\n    width = 0.6\n    \n    bars = axes[0, 0].bar(x, metrics['per_class_precision'], width, \n                          color='#3498DB', edgecolor='black', linewidth=1.2, alpha=0.85)\n    axes[0, 0].set_ylabel('Precision', fontsize=12, fontweight='bold')\n    axes[0, 0].set_title('Precision per Class', fontsize=14, fontweight='bold', pad=12)\n    axes[0, 0].set_xticks(x)\n    axes[0, 0].set_xticklabels(class_names, rotation=45, ha='right')\n    axes[0, 0].set_ylim([0, 1.1])\n    axes[0, 0].grid(True, alpha=0.3, axis='y', linestyle='--')\n    for bar in bars:\n        height = bar.get_height()\n        axes[0, 0].text(bar.get_x() + bar.get_width()/2., height,\n                        f'{height:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n    \n    bars = axes[0, 1].bar(x, metrics['per_class_recall'], width, \n                          color='#2ECC71', edgecolor='black', linewidth=1.2, alpha=0.85)\n    axes[0, 1].set_ylabel('Recall', fontsize=12, fontweight='bold')\n    axes[0, 1].set_title('Recall per Class', fontsize=14, fontweight='bold', pad=12)\n    axes[0, 1].set_xticks(x)\n    axes[0, 1].set_xticklabels(class_names, rotation=45, ha='right')\n    axes[0, 1].set_ylim([0, 1.1])\n    axes[0, 1].grid(True, alpha=0.3, axis='y', linestyle='--')\n    for bar in bars:\n        height = bar.get_height()\n        axes[0, 1].text(bar.get_x() + bar.get_width()/2., height,\n                        f'{height:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n    \n    bars = axes[1, 0].bar(x, metrics['per_class_f1'], width, \n                          color='#F39C12', edgecolor='black', linewidth=1.2, alpha=0.85)\n    axes[1, 0].set_ylabel('F1-Score', fontsize=12, fontweight='bold')\n    axes[1, 0].set_title('F1-Score per Class', fontsize=14, fontweight='bold', pad=12)\n    axes[1, 0].set_xticks(x)\n    axes[1, 0].set_xticklabels(class_names, rotation=45, ha='right')\n    axes[1, 0].set_ylim([0, 1.1])\n    axes[1, 0].grid(True, alpha=0.3, axis='y', linestyle='--')\n    for bar in bars:\n        height = bar.get_height()\n        axes[1, 0].text(bar.get_x() + bar.get_width()/2., height,\n                        f'{height:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n    \n    bars = axes[1, 1].bar(x, metrics['per_class_support'], width, \n                          color='#E74C3C', edgecolor='black', linewidth=1.2, alpha=0.85)\n    axes[1, 1].set_ylabel('Support (# samples)', fontsize=12, fontweight='bold')\n    axes[1, 1].set_title('Support per Class', fontsize=14, fontweight='bold', pad=12)\n    axes[1, 1].set_xticks(x)\n    axes[1, 1].set_xticklabels(class_names, rotation=45, ha='right')\n    axes[1, 1].grid(True, alpha=0.3, axis='y', linestyle='--')\n    for bar in bars:\n        height = bar.get_height()\n        axes[1, 1].text(bar.get_x() + bar.get_width()/2., height,\n                        f'{int(height)}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n    \n    plt.suptitle(f'Per-Class Performance Metrics - {model_name}', \n                 fontsize=16, fontweight='bold', y=1.00)\n    plt.tight_layout()\n    plt.savefig(f'{model_name}_per_class_metrics.png', dpi=300, bbox_inches='tight', facecolor='white')\n    plt.close()\n\ndef plot_overall_metrics(metrics, model_name):\n    metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n    metric_values = [metrics['accuracy'], metrics['precision'], \n                     metrics['recall'], metrics['f1_score']]\n    colors = ['#2ECC71', '#3498DB', '#E74C3C', '#F39C12']\n    \n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n    \n    bars = ax1.bar(metric_names, metric_values, color=colors,\n                   edgecolor='black', linewidth=1.5, alpha=0.85)\n    \n    for bar in bars:\n        height = bar.get_height()\n        ax1.text(bar.get_x() + bar.get_width()/2., height,\n                f'{height:.4f}',\n                ha='center', va='bottom', fontsize=12, fontweight='bold')\n    \n    ax1.set_ylabel('Score', fontsize=13, fontweight='bold')\n    ax1.set_title(f'Overall Performance Metrics - {model_name}', fontsize=15, fontweight='bold', pad=15)\n    ax1.set_ylim([0, 1.1])\n    ax1.grid(True, alpha=0.3, axis='y', linestyle='--')\n    ax1.tick_params(axis='x', labelsize=11)\n    \n    sizes = metric_values\n    explode = (0.05, 0.05, 0.05, 0.05)\n    \n    wedges, texts, autotexts = ax2.pie(sizes, explode=explode, labels=metric_names,\n                                         colors=colors, autopct='%1.2f%%',\n                                         shadow=True, startangle=90,\n                                         textprops={'fontsize': 11, 'fontweight': 'bold'})\n    \n    for autotext in autotexts:\n        autotext.set_color('white')\n        autotext.set_fontsize(11)\n        autotext.set_fontweight('bold')\n    \n    ax2.set_title(f'Metrics Distribution - {model_name}', fontsize=15, fontweight='bold', pad=15)\n    \n    plt.tight_layout()\n    plt.savefig(f'{model_name}_overall_metrics.png', dpi=300, bbox_inches='tight', facecolor='white')\n    plt.close()\n\ndef plot_roc_curves(metrics, class_names, model_name):\n    n_classes = len(class_names)\n    y_true_bin = label_binarize(metrics['labels'], classes=range(n_classes))\n    y_score = metrics['probabilities']\n    \n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    \n    for i in range(n_classes):\n        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_score[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n    \n    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), y_score.ravel())\n    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n    \n    plt.figure(figsize=(12, 9))\n    \n    colors = ['#E74C3C', '#3498DB', '#2ECC71', '#F39C12', '#9B59B6']\n    \n    for i, color in zip(range(n_classes), colors):\n        plt.plot(fpr[i], tpr[i], color=color, lw=2.5,\n                 label=f'{class_names[i]} (AUC = {roc_auc[i]:.3f})')\n    \n    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n             label=f'Micro-average (AUC = {roc_auc[\"micro\"]:.3f})',\n             color='navy', linestyle='--', linewidth=3)\n    \n    plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n    \n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate', fontsize=13, fontweight='bold')\n    plt.ylabel('True Positive Rate', fontsize=13, fontweight='bold')\n    plt.title(f'ROC Curves - {model_name}', fontsize=15, fontweight='bold', pad=15)\n    plt.legend(loc=\"lower right\", fontsize=11, frameon=True, shadow=True)\n    plt.grid(True, alpha=0.3, linestyle='--')\n    plt.tight_layout()\n    plt.savefig(f'{model_name}_roc_curves.png', dpi=300, bbox_inches='tight', facecolor='white')\n    plt.close()\n\ndef plot_model_comparison(baseline_metrics, watermarked_metrics, class_names):\n    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n    \n    metrics_overall = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n    baseline_vals = [baseline_metrics['accuracy'], baseline_metrics['precision'],\n                     baseline_metrics['recall'], baseline_metrics['f1_score']]\n    watermarked_vals = [watermarked_metrics['accuracy'], watermarked_metrics['precision'],\n                        watermarked_metrics['recall'], watermarked_metrics['f1_score']]\n    \n    x = np.arange(len(metrics_overall))\n    width = 0.35\n    \n    bars1 = axes[0, 0].bar(x - width/2, baseline_vals, width, label='Baseline',\n                           color='#95A5A6', edgecolor='black', linewidth=1.2, alpha=0.85)\n    bars2 = axes[0, 0].bar(x + width/2, watermarked_vals, width, label='Watermarked',\n                           color='#E74C3C', edgecolor='black', linewidth=1.2, alpha=0.85)\n    \n    axes[0, 0].set_ylabel('Score', fontsize=12, fontweight='bold')\n    axes[0, 0].set_title('Overall Performance Comparison', fontsize=14, fontweight='bold', pad=12)\n    axes[0, 0].set_xticks(x)\n    axes[0, 0].set_xticklabels(metrics_overall, fontsize=11)\n    axes[0, 0].legend(fontsize=11, frameon=True, shadow=True)\n    axes[0, 0].set_ylim([0, 1.1])\n    axes[0, 0].grid(True, alpha=0.3, axis='y', linestyle='--')\n    \n    for bars in [bars1, bars2]:\n        for bar in bars:\n            height = bar.get_height()\n            axes[0, 0].text(bar.get_x() + bar.get_width()/2., height,\n                           f'{height:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n    \n    performance_drops = [(baseline_vals[i] - watermarked_vals[i]) * 100 for i in range(len(baseline_vals))]\n    \n    bars = axes[0, 1].bar(metrics_overall, performance_drops,\n                          color=['#E74C3C' if x > 0 else '#2ECC71' for x in performance_drops],\n                          edgecolor='black', linewidth=1.2, alpha=0.85)\n    \n    axes[0, 1].axhline(y=0, color='black', linestyle='-', linewidth=1)\n    axes[0, 1].axhline(y=1, color='green', linestyle='--', linewidth=2, alpha=0.7, label='Target (1%)')\n    axes[0, 1].set_ylabel('Performance Drop (%)', fontsize=12, fontweight='bold')\n    axes[0, 1].set_title('Watermarking Impact', fontsize=14, fontweight='bold', pad=12)\n    axes[0, 1].legend(fontsize=11)\n    axes[0, 1].grid(True, alpha=0.3, axis='y', linestyle='--')\n    \n    for bar in bars:\n        height = bar.get_height()\n        axes[0, 1].text(bar.get_x() + bar.get_width()/2., height,\n                       f'{height:.2f}%', ha='center', \n                       va='bottom' if height >= 0 else 'top', fontsize=10, fontweight='bold')\n    \n    x_classes = np.arange(len(class_names))\n    width = 0.35\n    \n    bars1 = axes[1, 0].bar(x_classes - width/2, baseline_metrics['per_class_f1'], width,\n                           label='Baseline', color='#95A5A6', edgecolor='black', linewidth=1.2, alpha=0.85)\n    bars2 = axes[1, 0].bar(x_classes + width/2, watermarked_metrics['per_class_f1'], width,\n                           label='Watermarked', color='#E74C3C', edgecolor='black', linewidth=1.2, alpha=0.85)\n    \n    axes[1, 0].set_ylabel('F1-Score', fontsize=12, fontweight='bold')\n    axes[1, 0].set_title('Per-Class F1-Score Comparison', fontsize=14, fontweight='bold', pad=12)\n    axes[1, 0].set_xticks(x_classes)\n    axes[1, 0].set_xticklabels(class_names, rotation=45, ha='right', fontsize=11)\n    axes[1, 0].legend(fontsize=11, frameon=True, shadow=True)\n    axes[1, 0].set_ylim([0, 1.1])\n    axes[1, 0].grid(True, alpha=0.3, axis='y', linestyle='--')\n    \n    class_drops = [(baseline_metrics['per_class_f1'][i] - watermarked_metrics['per_class_f1'][i]) * 100 \n                   for i in range(len(class_names))]\n    \n    bars = axes[1, 1].bar(class_names, class_drops,\n                          color=['#E74C3C' if x > 0 else '#2ECC71' for x in class_drops],\n                          edgecolor='black', linewidth=1.2, alpha=0.85)\n    \n    axes[1, 1].axhline(y=0, color='black', linestyle='-', linewidth=1)\n    axes[1, 1].set_ylabel('F1-Score Drop (%)', fontsize=12, fontweight='bold')\n    axes[1, 1].set_title('Per-Class Performance Impact', fontsize=14, fontweight='bold', pad=12)\n    axes[1, 1].set_xticklabels(class_names, rotation=45, ha='right', fontsize=11)\n    axes[1, 1].grid(True, alpha=0.3, axis='y', linestyle='--')\n    \n    for bar in bars:\n        height = bar.get_height()\n        axes[1, 1].text(bar.get_x() + bar.get_width()/2., height,\n                       f'{height:.1f}%', ha='center', \n                       va='bottom' if height >= 0 else 'top', fontsize=9, fontweight='bold')\n    \n    plt.suptitle('Baseline vs Watermarked Model Comparison', \n                 fontsize=16, fontweight='bold', y=1.00)\n    plt.tight_layout()\n    plt.savefig('model_comparison_comprehensive.png', dpi=300, bbox_inches='tight', facecolor='white')\n    plt.close()\n\ndef plot_watermark_verification(baseline_verification, watermarked_verification):\n    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n    \n    models = ['Baseline', 'Watermarked']\n    match_ratios = [0, watermarked_verification['match_ratio']]\n    \n    bars = axes[0].bar(models, match_ratios, \n                       color=['#95A5A6', '#E74C3C'],\n                       edgecolor='black', linewidth=1.5, alpha=0.85, width=0.5)\n    \n    axes[0].axhline(y=0.95, color='green', linestyle='--', linewidth=2.5, \n                    label='Detection Threshold (0.95)', alpha=0.8)\n    \n    axes[0].set_ylabel('Weight Match Ratio', fontsize=13, fontweight='bold')\n    axes[0].set_title('Fixed Weight Watermark Verification', fontsize=15, fontweight='bold', pad=15)\n    axes[0].set_ylim([0, 1.1])\n    axes[0].legend(fontsize=12, frameon=True, shadow=True)\n    axes[0].grid(True, alpha=0.3, axis='y', linestyle='--')\n    \n    for bar in bars:\n        height = bar.get_height()\n        if height > 0:\n            axes[0].text(bar.get_x() + bar.get_width()/2., height,\n                        f'{height:.4f}',\n                        ha='center', va='bottom', fontsize=12, fontweight='bold')\n    \n    detected_status = ['Not Detected', 'Detected']\n    colors_status = ['#95A5A6', '#2ECC71']\n    counts = [1, 1]\n    \n    wedges, texts, autotexts = axes[1].pie(counts, labels=detected_status,\n                                            colors=colors_status, autopct='',\n                                            shadow=True, startangle=90,\n                                            explode=(0.05, 0.05),\n                                            textprops={'fontsize': 13, 'fontweight': 'bold'})\n    \n    axes[1].set_title('Watermark Detection Status', fontsize=15, fontweight='bold', pad=15)\n    \n    detection_text = f\"Baseline: Not Detected\\nWatermarked: {'Detected ✓' if watermarked_verification['detected'] else 'Not Detected ✗'}\"\n    axes[1].text(0, -1.4, detection_text, ha='center', fontsize=12, \n                 bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n    \n    plt.tight_layout()\n    plt.savefig('watermark_verification.png', dpi=300, bbox_inches='tight', facecolor='white')\n    plt.close()\n\ndef plot_watermark_statistics(watermark_stats):\n    layer_names = list(watermark_stats['layer_details'].keys())\n    num_frozen = [watermark_stats['layer_details'][name]['num_frozen'] for name in layer_names]\n    \n    short_names = [name.split('.')[-2] + '.' + name.split('.')[-1] if '.' in name else name \n                   for name in layer_names]\n    \n    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n    \n    bars = axes[0].barh(short_names, num_frozen, color='#3498DB', \n                        edgecolor='black', linewidth=1.2, alpha=0.85)\n    \n    axes[0].set_xlabel('Number of Frozen Weights', fontsize=13, fontweight='bold')\n    axes[0].set_ylabel('Layer', fontsize=13, fontweight='bold')\n    axes[0].set_title('Frozen Weights Distribution Across Layers', fontsize=15, fontweight='bold', pad=15)\n    axes[0].grid(True, alpha=0.3, axis='x', linestyle='--')\n    \n    for i, bar in enumerate(bars):\n        width = bar.get_width()\n        axes[0].text(width, bar.get_y() + bar.get_height()/2.,\n                    f' {int(width)}', ha='left', va='center', fontsize=10, fontweight='bold')\n    \n    total_frozen = watermark_stats['total_frozen_weights']\n    total_layers = watermark_stats['total_layers']\n    \n    info_text = f\"Total Frozen Weights: {total_frozen}\\n\"\n    info_text += f\"Total Layers: {total_layers}\\n\"\n    info_text += f\"Average per Layer: {total_frozen/total_layers:.1f}\"\n    \n    axes[1].text(0.5, 0.6, info_text, ha='center', va='center',\n                fontsize=14, fontweight='bold',\n                bbox=dict(boxstyle='round', facecolor='#ECF0F1', alpha=0.8, pad=1.5),\n                transform=axes[1].transAxes)\n    \n    axes[1].text(0.5, 0.3, f\"Watermark Configuration\", ha='center', va='center',\n                fontsize=16, fontweight='bold', style='italic',\n                transform=axes[1].transAxes)\n    \n    axes[1].axis('off')\n    \n    plt.tight_layout()\n    plt.savefig('watermark_statistics.png', dpi=300, bbox_inches='tight', facecolor='white')\n    plt.close()\n\ndef save_comprehensive_results(baseline_metrics, watermarked_metrics, \n                                baseline_verification, watermarked_verification,\n                                watermark_stats, class_names):\n    \n    results = {\n        'Dataset': {\n            'Name': 'Flowers Recognition',\n            'Classes': class_names,\n            'Num_Classes': len(class_names)\n        },\n        'Baseline_Model': {\n            'Performance': {\n                'Accuracy': float(baseline_metrics['accuracy']),\n                'Precision': float(baseline_metrics['precision']),\n                'Recall': float(baseline_metrics['recall']),\n                'F1_Score': float(baseline_metrics['f1_score'])\n            },\n            'Per_Class_Performance': {\n                class_names[i]: {\n                    'Precision': float(baseline_metrics['per_class_precision'][i]),\n                    'Recall': float(baseline_metrics['per_class_recall'][i]),\n                    'F1_Score': float(baseline_metrics['per_class_f1'][i]),\n                    'Support': int(baseline_metrics['per_class_support'][i])\n                }\n                for i in range(len(class_names))\n            },\n            'Verification': {\n                'Match_Ratio': float(baseline_verification['match_ratio']),\n                'Detected': bool(baseline_verification['detected'])\n            }\n        },\n        'Watermarked_Model': {\n            'Performance': {\n                'Accuracy': float(watermarked_metrics['accuracy']),\n                'Precision': float(watermarked_metrics['precision']),\n                'Recall': float(watermarked_metrics['recall']),\n                'F1_Score': float(watermarked_metrics['f1_score'])\n            },\n            'Per_Class_Performance': {\n                class_names[i]: {\n                    'Precision': float(watermarked_metrics['per_class_precision'][i]),\n                    'Recall': float(watermarked_metrics['per_class_recall'][i]),\n                    'F1_Score': float(watermarked_metrics['per_class_f1'][i]),\n                    'Support': int(watermarked_metrics['per_class_support'][i])\n                }\n                for i in range(len(class_names))\n            },\n            'Verification': {\n                'Match_Ratio': float(watermarked_verification['match_ratio']),\n                'Total_Frozen_Weights': int(watermarked_verification['total_frozen']),\n                'Detected': bool(watermarked_verification['detected'])\n            },\n            'Watermark_Configuration': watermark_stats\n        },\n        'Performance_Impact': {\n            'Overall': {\n                'Accuracy_Drop': float(baseline_metrics['accuracy'] - watermarked_metrics['accuracy']),\n                'Precision_Drop': float(baseline_metrics['precision'] - watermarked_metrics['precision']),\n                'Recall_Drop': float(baseline_metrics['recall'] - watermarked_metrics['recall']),\n                'F1_Score_Drop': float(baseline_metrics['f1_score'] - watermarked_metrics['f1_score'])\n            },\n            'Per_Class_F1_Drop': {\n                class_names[i]: float(baseline_metrics['per_class_f1'][i] - watermarked_metrics['per_class_f1'][i])\n                for i in range(len(class_names))\n            }\n        }\n    }\n    \n    with open('fixed_watermark_results.json', 'w') as f:\n        json.dump(results, f, indent=4)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"COMPREHENSIVE EXPERIMENTAL RESULTS\")\n    print(\"=\"*80)\n    \n    print(\"\\n\" + \"-\"*80)\n    print(\"BASELINE MODEL (No Watermark)\")\n    print(\"-\"*80)\n    print(f\"  Overall Performance:\")\n    print(f\"    • Accuracy:  {baseline_metrics['accuracy']:.4f}\")\n    print(f\"    • Precision: {baseline_metrics['precision']:.4f}\")\n    print(f\"    • Recall:    {baseline_metrics['recall']:.4f}\")\n    print(f\"    • F1-Score:  {baseline_metrics['f1_score']:.4f}\")\n    print(f\"  Watermark Status: {'Detected (Unexpected!)' if baseline_verification['detected'] else 'Not Detected ✓'}\")\n    \n    print(\"\\n\" + \"-\"*80)\n    print(\"WATERMARKED MODEL (Fixed Weight Watermark)\")\n    print(\"-\"*80)\n    print(f\"  Overall Performance:\")\n    print(f\"    • Accuracy:  {watermarked_metrics['accuracy']:.4f}\")\n    print(f\"    • Precision: {watermarked_metrics['precision']:.4f}\")\n    print(f\"    • Recall:    {watermarked_metrics['recall']:.4f}\")\n    print(f\"    • F1-Score:  {watermarked_metrics['f1_score']:.4f}\")\n    print(f\"\\n  Watermark Configuration:\")\n    print(f\"    • Total Frozen Weights: {watermark_stats['total_frozen_weights']}\")\n    print(f\"    • Frozen Layers: {watermark_stats['total_layers']}\")\n    print(f\"    • Match Ratio: {watermarked_verification['match_ratio']:.4f}\")\n    print(f\"    • Detection Status: {'Detected ✓' if watermarked_verification['detected'] else 'Not Detected ✗'}\")\n    \n    print(\"\\n\" + \"-\"*80)\n    print(\"PERFORMANCE IMPACT ANALYSIS\")\n    print(\"-\"*80)\n    acc_drop = (baseline_metrics['accuracy'] - watermarked_metrics['accuracy']) * 100\n    print(f\"  • Accuracy Drop:  {acc_drop:+.2f}%\")\n    print(f\"  • Precision Drop: {(baseline_metrics['precision'] - watermarked_metrics['precision']) * 100:+.2f}%\")\n    print(f\"  • Recall Drop:    {(baseline_metrics['recall'] - watermarked_metrics['recall']) * 100:+.2f}%\")\n    print(f\"  • F1-Score Drop:  {(baseline_metrics['f1_score'] - watermarked_metrics['f1_score']) * 100:+.2f}%\")\n    \n    if abs(acc_drop) <= 1.0:\n        print(f\"\\n  ✓ Performance impact is within acceptable range (≤1%)\")\n    else:\n        print(f\"\\n  ⚠ Performance impact exceeds target threshold (>1%)\")\n    \n    print(\"\\n\" + \"-\"*80)\n    print(\"PER-CLASS PERFORMANCE (F1-Score)\")\n    print(\"-\"*80)\n    for i, class_name in enumerate(class_names):\n        baseline_f1 = baseline_metrics['per_class_f1'][i]\n        watermarked_f1 = watermarked_metrics['per_class_f1'][i]\n        drop = (baseline_f1 - watermarked_f1) * 100\n        print(f\"  {class_name:12s}: Baseline={baseline_f1:.4f}, Watermarked={watermarked_f1:.4f}, Drop={drop:+.2f}%\")\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"CONCLUSION\")\n    print(\"=\"*80)\n    \n    if watermarked_verification['detected'] and not baseline_verification['detected']:\n        print(\"  ✓ Watermark successfully embedded and detected\")\n        print(f\"  ✓ Match ratio: {watermarked_verification['match_ratio']:.4f} (above threshold)\")\n    else:\n        print(\"  ⚠ Watermark detection issue\")\n    \n    if abs(acc_drop) <= 1.0:\n        print(\"  ✓ Minimal performance degradation achieved\")\n    else:\n        print(\"  ⚠ Performance degradation exceeds target\")\n    \n    print(\"=\"*80 + \"\\n\")\n\ndef main():\n    print(\"\\n\" + \"=\"*80)\n    print(\"FIXED WEIGHT WATERMARKING FOR MODEL ATTRIBUTION\")\n    print(\"Flower Recognition Dataset - Enhanced ResNet Architecture\")\n    print(\"=\"*80 + \"\\n\")\n    \n    BASE_PATH = '/kaggle/input/flowers-recognition/flowers'\n    FINGERPRINT = \"FlowerCNN_FixedWatermark_2025_SecureOwnership\"\n    \n    IMG_SIZE = 224\n    BATCH_SIZE = 32\n    NUM_EPOCHS = 10\n    LEARNING_RATE = 0.001\n    FREEZE_RATIO = 0.002\n    \n    print(\"Loading Flower Dataset...\")\n    all_paths, all_labels, class_names = load_flower_dataset(BASE_PATH)\n    \n    train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n        all_paths, all_labels, test_size=0.3, stratify=all_labels, random_state=RANDOM_SEED\n    )\n    val_paths, test_paths, val_labels, test_labels = train_test_split(\n        temp_paths, temp_labels, test_size=0.5, stratify=temp_labels, random_state=RANDOM_SEED\n    )\n    \n    print(f\"\\nData Split:\")\n    print(f\"  Train:      {len(train_paths)} images\")\n    print(f\"  Validation: {len(val_paths)} images\")\n    print(f\"  Test:       {len(test_paths)} images\")\n    \n    transform_train = transforms.Compose([\n        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.RandomVerticalFlip(p=0.3),\n        transforms.RandomRotation(20),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    transform_test = transforms.Compose([\n        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    train_dataset = FlowerDataset(train_paths, train_labels, transform=transform_train)\n    val_dataset = FlowerDataset(val_paths, val_labels, transform=transform_test)\n    test_dataset = FlowerDataset(test_paths, test_labels, transform=transform_test)\n    \n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"EXPERIMENT 1: BASELINE MODEL (No Watermark)\")\n    print(\"=\"*80)\n    \n    baseline_model = FlowerCNN(num_classes=len(class_names)).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(baseline_model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, \n                                                       patience=3, verbose=True)\n    \n    baseline_history = train_model(\n        baseline_model, train_loader, val_loader, criterion, optimizer, scheduler,\n        NUM_EPOCHS, watermark_handler=None, model_name=\"baseline\"\n    )\n    \n    baseline_model.load_state_dict(torch.load('baseline_best.pth'))\n    \n    print(\"\\nEvaluating Baseline Model...\")\n    baseline_metrics = evaluate_model(baseline_model, test_loader, class_names)\n    print(\"\\n\" + baseline_metrics['classification_report'])\n    \n    print(\"\\nGenerating Baseline Model Visualizations...\")\n    plot_training_history(baseline_history, \"Baseline\")\n    plot_confusion_matrix(baseline_metrics['confusion_matrix'], class_names, \"Baseline\")\n    plot_per_class_metrics(baseline_metrics, class_names, \"Baseline\")\n    plot_overall_metrics(baseline_metrics, \"Baseline\")\n    plot_roc_curves(baseline_metrics, class_names, \"Baseline\")\n    \n    print(\"\\nVerifying Baseline Model (Should Not Detect Watermark)...\")\n    dummy_watermark_baseline = FixedWeightWatermark(baseline_model, FINGERPRINT, FREEZE_RATIO)\n    dummy_watermark_baseline.generate_frozen_pattern()\n    match_ratio, total_frozen, _ = dummy_watermark_baseline.verify_frozen_weights()\n    \n    baseline_verification = {\n        'match_ratio': match_ratio,\n        'total_frozen': total_frozen,\n        'detected': match_ratio > 0.95\n    }\n    \n    print(f\"  Match Ratio: {match_ratio:.4f}\")\n    print(f\"  Detected: {'YES (Unexpected!)' if baseline_verification['detected'] else 'NO ✓'}\")\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"EXPERIMENT 2: WATERMARKED MODEL (Fixed Weight Watermark)\")\n    print(\"=\"*80)\n    print(f\"  Fingerprint: {FINGERPRINT}\")\n    print(f\"  Freeze Ratio: {FREEZE_RATIO:.4f}\")\n    \n    watermarked_model = FlowerCNN(num_classes=len(class_names)).to(device)\n    watermark_handler = FixedWeightWatermark(watermarked_model, FINGERPRINT, FREEZE_RATIO)\n    watermark_handler.generate_frozen_pattern()\n    watermark_handler.apply_frozen_weights()\n    watermark_handler.freeze_gradient_hook()\n    \n    watermark_stats = watermark_handler.get_watermark_statistics()\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(watermarked_model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, \n                                                       patience=3, verbose=True)\n    \n    watermarked_history = train_model(\n        watermarked_model, train_loader, val_loader, criterion, optimizer, scheduler,\n        NUM_EPOCHS, watermark_handler=watermark_handler, model_name=\"watermarked\"\n    )\n    \n    watermarked_model.load_state_dict(torch.load('watermarked_best.pth'))\n    \n    print(\"\\nEvaluating Watermarked Model...\")\n    watermarked_metrics = evaluate_model(watermarked_model, test_loader, class_names)\n    print(\"\\n\" + watermarked_metrics['classification_report'])\n    \n    print(\"\\nGenerating Watermarked Model Visualizations...\")\n    plot_training_history(watermarked_history, \"Watermarked\")\n    plot_confusion_matrix(watermarked_metrics['confusion_matrix'], class_names, \"Watermarked\")\n    plot_per_class_metrics(watermarked_metrics, class_names, \"Watermarked\")\n    plot_overall_metrics(watermarked_metrics, \"Watermarked\")\n    plot_roc_curves(watermarked_metrics, class_names, \"Watermarked\")\n    \n    print(\"\\nVerifying Watermarked Model...\")\n    match_ratio, total_frozen, layer_matches = watermark_handler.verify_frozen_weights()\n    \n    watermarked_verification = {\n        'match_ratio': match_ratio,\n        'total_frozen': total_frozen,\n        'layer_matches': layer_matches,\n        'detected': match_ratio > 0.95\n    }\n    \n    print(f\"  Match Ratio: {match_ratio:.4f}\")\n    print(f\"  Total Frozen Weights: {total_frozen}\")\n    print(f\"  Detected: {'YES ✓' if watermarked_verification['detected'] else 'NO ✗'}\")\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"GENERATING COMPARISON VISUALIZATIONS\")\n    print(\"=\"*80)\n    \n    plot_model_comparison(baseline_metrics, watermarked_metrics, class_names)\n    plot_watermark_verification(baseline_verification, watermarked_verification)\n    plot_watermark_statistics(watermark_stats)\n    \n    save_comprehensive_results(baseline_metrics, watermarked_metrics,\n                               baseline_verification, watermarked_verification,\n                               watermark_stats, class_names)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"EXPERIMENT COMPLETED SUCCESSFULLY\")\n    print(\"=\"*80)\n    print(\"\\nGenerated Files:\")\n    print(\"\\n  Trained Models:\")\n    print(\"    • baseline_best.pth\")\n    print(\"    • watermarked_best.pth\")\n    print(\"\\n  Baseline Model Plots:\")\n    print(\"    • baseline_training_history.png\")\n    print(\"    • baseline_confusion_matrix.png\")\n    print(\"    • baseline_confusion_matrix_normalized.png\")\n    print(\"    • baseline_per_class_metrics.png\")\n    print(\"    • baseline_overall_metrics.png\")\n    print(\"    • baseline_roc_curves.png\")\n    print(\"\\n  Watermarked Model Plots:\")\n    print(\"    • watermarked_training_history.png\")\n    print(\"    • watermarked_confusion_matrix.png\")\n    print(\"    • watermarked_confusion_matrix_normalized.png\")\n    print(\"    • watermarked_per_class_metrics.png\")\n    print(\"    • watermarked_overall_metrics.png\")\n    print(\"    • watermarked_roc_curves.png\")\n    print(\"\\n  Comparison & Analysis Plots:\")\n    print(\"    • model_comparison_comprehensive.png\")\n    print(\"    • watermark_verification.png\")\n    print(\"    • watermark_statistics.png\")\n    print(\"\\n  Results Data:\")\n    print(\"    • fixed_watermark_results.json\")\n    print(\"\\n  Total: 17 PNG files + 1 JSON file + 2 model files\")\n    print(\"=\"*80 + \"\\n\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-22T09:01:52.619090Z","iopub.execute_input":"2025-10-22T09:01:52.619345Z","iopub.status.idle":"2025-10-22T09:12:02.726458Z","shell.execute_reply.started":"2025-10-22T09:01:52.619326Z","shell.execute_reply":"2025-10-22T09:12:02.725612Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n\n================================================================================\nFIXED WEIGHT WATERMARKING FOR MODEL ATTRIBUTION\nFlower Recognition Dataset - Enhanced ResNet Architecture\n================================================================================\n\nLoading Flower Dataset...\n\nDataset Distribution:\n----------------------------------------\n  daisy       :  764 images\n  dandelion   : 1052 images\n  rose        :  784 images\n  sunflower   :  733 images\n  tulip       :  984 images\n----------------------------------------\n  Total:        4317 images\n----------------------------------------\n\nData Split:\n  Train:      3021 images\n  Validation: 648 images\n  Test:       648 images\n\n================================================================================\nEXPERIMENT 1: BASELINE MODEL (No Watermark)\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10 [baseline]: 100%|██████████| 95/95 [00:27<00:00,  3.39it/s, loss=1.46, acc=35]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss: 1.4577, Train Acc: 34.96%, Val Loss: 1.4902, Val Acc: 42.75%, LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10 [baseline]: 100%|██████████| 95/95 [00:22<00:00,  4.14it/s, loss=1.39, acc=40.1] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss: 1.3875, Train Acc: 40.09%, Val Loss: 1.3553, Val Acc: 41.36%, LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10 [baseline]: 100%|██████████| 95/95 [00:23<00:00,  4.07it/s, loss=1.34, acc=42.7] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss: 1.3370, Train Acc: 42.67%, Val Loss: 1.3165, Val Acc: 40.43%, LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10 [baseline]: 100%|██████████| 95/95 [00:24<00:00,  3.96it/s, loss=1.31, acc=44.5] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss: 1.3117, Train Acc: 44.49%, Val Loss: 1.1838, Val Acc: 45.22%, LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10 [baseline]: 100%|██████████| 95/95 [00:24<00:00,  3.88it/s, loss=1.26, acc=45.2] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss: 1.2611, Train Acc: 45.15%, Val Loss: 1.1700, Val Acc: 52.16%, LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10 [baseline]: 100%|██████████| 95/95 [00:25<00:00,  3.78it/s, loss=1.23, acc=49.5] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Loss: 1.2341, Train Acc: 49.49%, Val Loss: 1.1738, Val Acc: 53.70%, LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10 [baseline]: 100%|██████████| 95/95 [00:25<00:00,  3.75it/s, loss=1.22, acc=48.6] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Loss: 1.2246, Train Acc: 48.59%, Val Loss: 1.1557, Val Acc: 53.40%, LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10 [baseline]: 100%|██████████| 95/95 [00:26<00:00,  3.64it/s, loss=1.18, acc=52.1] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Loss: 1.1798, Train Acc: 52.07%, Val Loss: 1.0650, Val Acc: 55.86%, LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10 [baseline]: 100%|██████████| 95/95 [00:26<00:00,  3.57it/s, loss=1.15, acc=54.7] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Loss: 1.1515, Train Acc: 54.65%, Val Loss: 1.0291, Val Acc: 58.95%, LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10 [baseline]: 100%|██████████| 95/95 [00:26<00:00,  3.53it/s, loss=1.11, acc=55.8]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss: 1.1055, Train Acc: 55.84%, Val Loss: 1.0950, Val Acc: 54.78%, LR: 0.001000\n\nEvaluating Baseline Model...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 21/21 [00:03<00:00,  5.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n              precision    recall  f1-score   support\n\n       daisy     0.4811    0.7739    0.5933       115\n   dandelion     0.6707    0.6962    0.6832       158\n        rose     0.4286    0.5128    0.4669       117\n   sunflower     0.6917    0.7545    0.7217       110\n       tulip     0.7436    0.1959    0.3102       148\n\n    accuracy                         0.5725       648\n   macro avg     0.6031    0.5867    0.5551       648\nweighted avg     0.6135    0.5725    0.5496       648\n\n\nGenerating Baseline Model Visualizations...\n\nVerifying Baseline Model (Should Not Detect Watermark)...\n\nWatermark Configuration:\n  Fingerprint: FlowerCNN_FixedWatermark_2025_SecureOwnership\n  Freeze Ratio: 0.0020\n  Total Frozen Weights: 90474\n  Frozen Layers: 45\n  Match Ratio: 0.0795\n  Detected: NO ✓\n\n================================================================================\nEXPERIMENT 2: WATERMARKED MODEL (Fixed Weight Watermark)\n================================================================================\n  Fingerprint: FlowerCNN_FixedWatermark_2025_SecureOwnership\n  Freeze Ratio: 0.0020\n\nWatermark Configuration:\n  Fingerprint: FlowerCNN_FixedWatermark_2025_SecureOwnership\n  Freeze Ratio: 0.0020\n  Total Frozen Weights: 90474\n  Frozen Layers: 45\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10 [watermarked]: 100%|██████████| 95/95 [00:27<00:00,  3.51it/s, loss=1.49, acc=33.8] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss: 1.4928, Train Acc: 33.80%, Val Loss: 1.2358, Val Acc: 45.99%, LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10 [watermarked]: 100%|██████████| 95/95 [00:27<00:00,  3.43it/s, loss=1.37, acc=40.1] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss: 1.3667, Train Acc: 40.09%, Val Loss: 1.2615, Val Acc: 44.14%, LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10 [watermarked]: 100%|██████████| 95/95 [00:27<00:00,  3.49it/s, loss=1.33, acc=41.9] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss: 1.3335, Train Acc: 41.87%, Val Loss: 1.2700, Val Acc: 42.90%, LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10 [watermarked]: 100%|██████████| 95/95 [00:27<00:00,  3.47it/s, loss=1.29, acc=42.8] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss: 1.2862, Train Acc: 42.83%, Val Loss: 1.1347, Val Acc: 48.46%, LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10 [watermarked]: 100%|██████████| 95/95 [00:27<00:00,  3.45it/s, loss=1.29, acc=43.1] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss: 1.2945, Train Acc: 43.13%, Val Loss: 1.1642, Val Acc: 47.84%, LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10 [watermarked]: 100%|██████████| 95/95 [00:27<00:00,  3.46it/s, loss=1.26, acc=45]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Loss: 1.2572, Train Acc: 44.95%, Val Loss: 1.1389, Val Acc: 48.77%, LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10 [watermarked]: 100%|██████████| 95/95 [00:27<00:00,  3.46it/s, loss=1.24, acc=45.3] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Loss: 1.2383, Train Acc: 45.35%, Val Loss: 1.1933, Val Acc: 47.22%, LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10 [watermarked]: 100%|██████████| 95/95 [00:27<00:00,  3.45it/s, loss=1.22, acc=48]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Loss: 1.2229, Train Acc: 48.03%, Val Loss: 1.2506, Val Acc: 46.76%, LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10 [watermarked]: 100%|██████████| 95/95 [00:27<00:00,  3.46it/s, loss=1.16, acc=52.1] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Loss: 1.1630, Train Acc: 52.14%, Val Loss: 1.0389, Val Acc: 54.94%, LR: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10 [watermarked]: 100%|██████████| 95/95 [00:27<00:00,  3.48it/s, loss=1.16, acc=51.4] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss: 1.1565, Train Acc: 51.37%, Val Loss: 1.0799, Val Acc: 55.40%, LR: 0.000500\n\nEvaluating Watermarked Model...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 21/21 [00:02<00:00, 10.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n              precision    recall  f1-score   support\n\n       daisy     0.5600    0.3652    0.4421       115\n   dandelion     0.5888    0.7342    0.6535       158\n        rose     0.4677    0.2479    0.3240       117\n   sunflower     0.5163    0.8636    0.6463       110\n       tulip     0.6231    0.5473    0.5827       148\n\n    accuracy                         0.5602       648\n   macro avg     0.5512    0.5516    0.5297       648\nweighted avg     0.5574    0.5602    0.5391       648\n\n\nGenerating Watermarked Model Visualizations...\n\nVerifying Watermarked Model...\n  Match Ratio: 1.0000\n  Total Frozen Weights: 90474\n  Detected: YES ✓\n\n================================================================================\nGENERATING COMPARISON VISUALIZATIONS\n================================================================================\n\n================================================================================\nCOMPREHENSIVE EXPERIMENTAL RESULTS\n================================================================================\n\n--------------------------------------------------------------------------------\nBASELINE MODEL (No Watermark)\n--------------------------------------------------------------------------------\n  Overall Performance:\n    • Accuracy:  0.5725\n    • Precision: 0.6135\n    • Recall:    0.5725\n    • F1-Score:  0.5496\n  Watermark Status: Not Detected ✓\n\n--------------------------------------------------------------------------------\nWATERMARKED MODEL (Fixed Weight Watermark)\n--------------------------------------------------------------------------------\n  Overall Performance:\n    • Accuracy:  0.5602\n    • Precision: 0.5574\n    • Recall:    0.5602\n    • F1-Score:  0.5391\n\n  Watermark Configuration:\n    • Total Frozen Weights: 90474\n    • Frozen Layers: 45\n    • Match Ratio: 1.0000\n    • Detection Status: Detected ✓\n\n--------------------------------------------------------------------------------\nPERFORMANCE IMPACT ANALYSIS\n--------------------------------------------------------------------------------\n  • Accuracy Drop:  +1.23%\n  • Precision Drop: +5.62%\n  • Recall Drop:    +1.23%\n  • F1-Score Drop:  +1.04%\n\n  ⚠ Performance impact exceeds target threshold (>1%)\n\n--------------------------------------------------------------------------------\nPER-CLASS PERFORMANCE (F1-Score)\n--------------------------------------------------------------------------------\n  daisy       : Baseline=0.5933, Watermarked=0.4421, Drop=+15.12%\n  dandelion   : Baseline=0.6832, Watermarked=0.6535, Drop=+2.97%\n  rose        : Baseline=0.4669, Watermarked=0.3240, Drop=+14.29%\n  sunflower   : Baseline=0.7217, Watermarked=0.6463, Drop=+7.55%\n  tulip       : Baseline=0.3102, Watermarked=0.5827, Drop=-27.26%\n\n================================================================================\nCONCLUSION\n================================================================================\n  ✓ Watermark successfully embedded and detected\n  ✓ Match ratio: 1.0000 (above threshold)\n  ⚠ Performance degradation exceeds target\n================================================================================\n\n\n================================================================================\nEXPERIMENT COMPLETED SUCCESSFULLY\n================================================================================\n\nGenerated Files:\n\n  Trained Models:\n    • baseline_best.pth\n    • watermarked_best.pth\n\n  Baseline Model Plots:\n    • baseline_training_history.png\n    • baseline_confusion_matrix.png\n    • baseline_confusion_matrix_normalized.png\n    • baseline_per_class_metrics.png\n    • baseline_overall_metrics.png\n    • baseline_roc_curves.png\n\n  Watermarked Model Plots:\n    • watermarked_training_history.png\n    • watermarked_confusion_matrix.png\n    • watermarked_confusion_matrix_normalized.png\n    • watermarked_per_class_metrics.png\n    • watermarked_overall_metrics.png\n    • watermarked_roc_curves.png\n\n  Comparison & Analysis Plots:\n    • model_comparison_comprehensive.png\n    • watermark_verification.png\n    • watermark_statistics.png\n\n  Results Data:\n    • fixed_watermark_results.json\n\n  Total: 17 PNG files + 1 JSON file + 2 model files\n================================================================================\n\n","output_type":"stream"}],"execution_count":1}]}