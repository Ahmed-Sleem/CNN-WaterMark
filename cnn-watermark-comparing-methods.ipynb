{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12289446,"sourceType":"datasetVersion","datasetId":3442424}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport hashlib\nimport json\nfrom scipy.stats import pearsonr\nfrom scipy.spatial.distance import cosine\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\nRANDOM_SEED = 42\ntorch.manual_seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\n\nclass ButterflyDataset(Dataset):\n    def __init__(self, df, img_dir, transform=None):\n        self.df = df.reset_index(drop=True)\n        self.img_dir = img_dir\n        self.transform = transform\n        self.labels = sorted(df['label'].unique())\n        self.label_to_idx = {label: idx for idx, label in enumerate(self.labels)}\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        img_name = self.df.loc[idx, 'filename']\n        label = self.df.loc[idx, 'label']\n        img_path = os.path.join(self.img_dir, img_name)\n        \n        image = Image.open(img_path).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n            \n        label_idx = self.label_to_idx[label]\n        \n        return image, label_idx\n\nclass SimpleCNN(nn.Module):\n    def __init__(self, num_classes):\n        super(SimpleCNN, self).__init__()\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(256 * 14 * 14, 512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(512, num_classes)\n        )\n        \n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\nclass FixedWeightWatermark:\n    def __init__(self, model, fingerprint, freeze_ratio=0.002):\n        self.model = model\n        self.fingerprint = fingerprint\n        self.freeze_ratio = freeze_ratio\n        self.frozen_params = {}\n        self.frozen_values = {}\n        \n    def generate_frozen_pattern(self):\n        hash_object = hashlib.sha256(self.fingerprint.encode())\n        hex_dig = hash_object.hexdigest()\n        seed = int(hex_dig[:8], 16)\n        rng = np.random.RandomState(seed)\n        \n        all_params = []\n        for name, param in self.model.named_parameters():\n            if 'weight' in name and 'features' in name:\n                all_params.append((name, param))\n        \n        for name, param in all_params:\n            num_weights = param.numel()\n            num_freeze = max(1, int(num_weights * self.freeze_ratio))\n            \n            indices = rng.choice(num_weights, size=num_freeze, replace=False)\n            \n            values = rng.randn(num_freeze) * 0.1\n            \n            self.frozen_params[name] = indices\n            self.frozen_values[name] = values\n    \n    def apply_frozen_weights(self):\n        with torch.no_grad():\n            for name, param in self.model.named_parameters():\n                if name in self.frozen_params:\n                    indices = self.frozen_params[name]\n                    values = self.frozen_values[name]\n                    \n                    param_flat = param.data.view(-1)\n                    param_flat[indices] = torch.FloatTensor(values).to(param.device)\n    \n    def freeze_gradient_hook(self):\n        def hook_fn(name):\n            def hook(grad):\n                if name in self.frozen_params:\n                    indices = self.frozen_params[name]\n                    grad_flat = grad.view(-1)\n                    grad_flat[indices] = 0.0\n                return grad\n            return hook\n        \n        for name, param in self.model.named_parameters():\n            if name in self.frozen_params:\n                param.register_hook(hook_fn(name))\n    \n    def verify_frozen_weights(self):\n        match_score = 0.0\n        total_frozen = 0\n        \n        with torch.no_grad():\n            for name, param in self.model.named_parameters():\n                if name in self.frozen_params:\n                    indices = self.frozen_params[name]\n                    expected_values = torch.FloatTensor(self.frozen_values[name]).to(param.device)\n                    \n                    param_flat = param.data.view(-1)\n                    actual_values = param_flat[indices]\n                    \n                    differences = torch.abs(actual_values - expected_values)\n                    matches = (differences < 0.01).sum().item()\n                    \n                    match_score += matches\n                    total_frozen += len(indices)\n        \n        match_ratio = match_score / total_frozen if total_frozen > 0 else 0.0\n        return match_ratio, total_frozen\n\nclass CausalWatermarkInjector:\n    def __init__(self, fingerprint, target_layers, lambda_factor=0.001):\n        self.fingerprint = fingerprint\n        self.lambda_factor = lambda_factor\n        self.target_layers = target_layers\n        self.fingerprint_vector = self._generate_fingerprint_vector()\n        \n    def _generate_fingerprint_vector(self):\n        hash_object = hashlib.sha256(self.fingerprint.encode())\n        hex_dig = hash_object.hexdigest()\n        \n        random_gen = np.random.RandomState(int(hex_dig[:8], 16))\n        vector = random_gen.randn(1000)\n        vector = vector / np.linalg.norm(vector)\n        \n        return torch.FloatTensor(vector).to(device)\n    \n    def inject_perturbation(self, model):\n        for name, param in model.named_parameters():\n            if any(layer_name in name for layer_name in self.target_layers):\n                if param.grad is not None:\n                    grad_flat = param.grad.view(-1)\n                    perturbation_size = min(len(grad_flat), len(self.fingerprint_vector))\n                    \n                    perturbation = self.fingerprint_vector[:perturbation_size]\n                    perturbation = perturbation * self.lambda_factor * torch.norm(grad_flat) / torch.norm(perturbation)\n                    \n                    param.grad.view(-1)[:perturbation_size] += perturbation\n\nclass HybridWatermark:\n    def __init__(self, model, fingerprint, freeze_ratio=0.002, lambda_factor=0.001):\n        self.fixed_watermark = FixedWeightWatermark(model, fingerprint, freeze_ratio)\n        self.causal_injector = CausalWatermarkInjector(\n            fingerprint, \n            target_layers=['features.0', 'features.3', 'features.6'],\n            lambda_factor=lambda_factor\n        )\n        \n    def initialize(self):\n        self.fixed_watermark.generate_frozen_pattern()\n        self.fixed_watermark.apply_frozen_weights()\n        self.fixed_watermark.freeze_gradient_hook()\n    \n    def inject_gradient_perturbation(self, model):\n        self.causal_injector.inject_perturbation(model)\n    \n    def maintain_frozen_weights(self):\n        self.fixed_watermark.apply_frozen_weights()\n    \n    def verify_fixed_weights(self):\n        return self.fixed_watermark.verify_frozen_weights()\n\ndef train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, \n                watermark_type=None, watermark_handler=None, model_name=\"model\"):\n    \n    train_losses = []\n    val_losses = []\n    train_accs = []\n    val_accs = []\n    \n    best_val_acc = 0.0\n    \n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [{model_name}]')\n        for inputs, labels in pbar:\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            \n            if watermark_type == 'causal' and watermark_handler is not None:\n                watermark_handler.inject_perturbation(model)\n            elif watermark_type == 'hybrid' and watermark_handler is not None:\n                watermark_handler.inject_gradient_perturbation(model)\n            \n            optimizer.step()\n            \n            if watermark_type == 'fixed' and watermark_handler is not None:\n                watermark_handler.apply_frozen_weights()\n            elif watermark_type == 'hybrid' and watermark_handler is not None:\n                watermark_handler.maintain_frozen_weights()\n            \n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n            \n            pbar.set_postfix({'loss': running_loss/len(pbar), 'acc': 100.*correct/total})\n        \n        train_loss = running_loss / len(train_loader)\n        train_acc = 100. * correct / total\n        train_losses.append(train_loss)\n        train_accs.append(train_acc)\n        \n        model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                \n                val_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n        \n        val_loss = val_loss / len(val_loader)\n        val_acc = 100. * correct / total\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n        \n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save(model.state_dict(), f'{model_name}_best.pth')\n        \n        print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, '\n              f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n    \n    history = {\n        'train_loss': train_losses,\n        'val_loss': val_losses,\n        'train_acc': train_accs,\n        'val_acc': val_accs\n    }\n    \n    return history\n\ndef evaluate_model(model, test_loader, class_names):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    all_probs = []\n    \n    with torch.no_grad():\n        for inputs, labels in tqdm(test_loader, desc='Evaluating'):\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            probs = torch.softmax(outputs, dim=1)\n            _, predicted = outputs.max(1)\n            \n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            all_probs.extend(probs.cpu().numpy())\n    \n    all_preds = np.array(all_preds)\n    all_labels = np.array(all_labels)\n    all_probs = np.array(all_probs)\n    \n    accuracy = accuracy_score(all_labels, all_preds)\n    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n    \n    cm = confusion_matrix(all_labels, all_preds)\n    \n    report = classification_report(all_labels, all_preds, target_names=class_names, digits=4)\n    \n    metrics = {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1,\n        'confusion_matrix': cm,\n        'predictions': all_preds,\n        'labels': all_labels,\n        'probabilities': all_probs,\n        'classification_report': report\n    }\n    \n    return metrics\n\ndef plot_training_history(history, model_name):\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    axes[0].plot(history['train_loss'], label='Train Loss', marker='o', linewidth=2)\n    axes[0].plot(history['val_loss'], label='Validation Loss', marker='s', linewidth=2)\n    axes[0].set_xlabel('Epoch', fontsize=12)\n    axes[0].set_ylabel('Loss', fontsize=12)\n    axes[0].set_title(f'Training and Validation Loss - {model_name}', fontsize=14, fontweight='bold')\n    axes[0].legend(fontsize=11)\n    axes[0].grid(True, alpha=0.3)\n    \n    axes[1].plot(history['train_acc'], label='Train Accuracy', marker='o', linewidth=2)\n    axes[1].plot(history['val_acc'], label='Validation Accuracy', marker='s', linewidth=2)\n    axes[1].set_xlabel('Epoch', fontsize=12)\n    axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n    axes[1].set_title(f'Training and Validation Accuracy - {model_name}', fontsize=14, fontweight='bold')\n    axes[1].legend(fontsize=11)\n    axes[1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(f'{model_name}_training_history.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    \ndef plot_confusion_matrix(cm, class_names, model_name):\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=class_names, yticklabels=class_names,\n                cbar_kws={'label': 'Count'})\n    plt.xlabel('Predicted Label', fontsize=12)\n    plt.ylabel('True Label', fontsize=12)\n    plt.title(f'Confusion Matrix - {model_name}', fontsize=14, fontweight='bold')\n    plt.xticks(rotation=45, ha='right')\n    plt.yticks(rotation=0)\n    plt.tight_layout()\n    plt.savefig(f'{model_name}_confusion_matrix.png', dpi=300, bbox_inches='tight')\n    plt.close()\n\ndef plot_metrics_comparison(metrics, model_name):\n    metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n    metric_values = [metrics['accuracy'], metrics['precision'], \n                     metrics['recall'], metrics['f1_score']]\n    \n    fig, ax = plt.subplots(figsize=(10, 6))\n    bars = ax.bar(metric_names, metric_values, color=['#2ecc71', '#3498db', '#e74c3c', '#f39c12'],\n                   edgecolor='black', linewidth=1.5, alpha=0.8)\n    \n    for bar in bars:\n        height = bar.get_height()\n        ax.text(bar.get_x() + bar.get_width()/2., height,\n                f'{height:.4f}',\n                ha='center', va='bottom', fontsize=11, fontweight='bold')\n    \n    ax.set_ylabel('Score', fontsize=12)\n    ax.set_title(f'Performance Metrics - {model_name}', fontsize=14, fontweight='bold')\n    ax.set_ylim([0, 1.1])\n    ax.grid(True, alpha=0.3, axis='y')\n    \n    plt.tight_layout()\n    plt.savefig(f'{model_name}_metrics.png', dpi=300, bbox_inches='tight')\n    plt.close()\n\nclass CausalWatermarkVerifier:\n    def __init__(self, model, num_probe_samples=100):\n        self.model = model\n        self.num_probe_samples = num_probe_samples\n        \n    def compute_integrated_gradients(self, inputs, target_class, steps=50):\n        self.model.eval()\n        \n        baseline = torch.zeros_like(inputs)\n        \n        scaled_inputs = torch.stack([baseline + (float(i) / steps) * (inputs - baseline) \n                                      for i in range(steps + 1)], dim=0)\n        \n        scaled_inputs = scaled_inputs.view(-1, *inputs.shape[1:])\n        scaled_inputs.requires_grad = True\n        \n        outputs = self.model(scaled_inputs)\n        target_outputs = outputs[:, target_class]\n        \n        gradients = torch.autograd.grad(outputs=target_outputs.sum(), \n                                         inputs=scaled_inputs,\n                                         create_graph=False)[0]\n        \n        avg_gradients = gradients.mean(dim=0)\n        integrated_grads = (inputs - baseline) * avg_gradients\n        \n        return integrated_grads\n    \n    def extract_attribution_signature(self, data_loader):\n        self.model.eval()\n        \n        all_attributions = []\n        samples_processed = 0\n        \n        for inputs, labels in data_loader:\n            if samples_processed >= self.num_probe_samples:\n                break\n                \n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            \n            outputs = self.model(inputs)\n            _, predicted = outputs.max(1)\n            \n            for i in range(inputs.size(0)):\n                if samples_processed >= self.num_probe_samples:\n                    break\n                    \n                single_input = inputs[i:i+1]\n                target_class = predicted[i].item()\n                \n                attributions = self.compute_integrated_gradients(single_input, target_class)\n                attribution_flat = attributions.cpu().detach().numpy().flatten()\n                \n                all_attributions.append(attribution_flat)\n                samples_processed += 1\n        \n        all_attributions = np.array(all_attributions)\n        \n        mean_attribution = np.mean(all_attributions, axis=0)\n        std_attribution = np.std(all_attributions, axis=0)\n        \n        top_k = 1000\n        top_indices = np.argsort(np.abs(mean_attribution))[-top_k:]\n        signature_vector = mean_attribution[top_indices]\n        signature_vector = signature_vector / (np.linalg.norm(signature_vector) + 1e-8)\n        \n        signature = {\n            'mean_attribution': mean_attribution,\n            'std_attribution': std_attribution,\n            'signature_vector': signature_vector,\n            'top_indices': top_indices\n        }\n        \n        return signature\n    \n    def verify_fingerprint(self, fingerprint, data_loader, fixed_watermark=None):\n        print(f\"\\n{'='*60}\")\n        print(f\"WATERMARK VERIFICATION\")\n        print(f\"{'='*60}\")\n        print(f\"Target Fingerprint: {fingerprint}\")\n        print(f\"Probe Samples: {self.num_probe_samples}\")\n        print(f\"{'-'*60}\")\n        \n        injector = CausalWatermarkInjector(fingerprint, target_layers=['features'])\n        expected_vector = injector.fingerprint_vector.cpu().numpy()\n        \n        signature = self.extract_attribution_signature(data_loader)\n        observed_vector = signature['signature_vector']\n        \n        expected_normalized = expected_vector[:len(observed_vector)]\n        expected_normalized = expected_normalized / (np.linalg.norm(expected_normalized) + 1e-8)\n        \n        cosine_sim = 1 - cosine(expected_normalized, observed_vector)\n        pearson_corr, p_value = pearsonr(expected_normalized, observed_vector)\n        \n        l2_distance = np.linalg.norm(expected_normalized - observed_vector)\n        \n        dot_product = np.dot(expected_normalized, observed_vector)\n        \n        causal_threshold = 0.15\n        causal_detected = cosine_sim > causal_threshold\n        \n        fixed_match_ratio = 0.0\n        fixed_total = 0\n        fixed_detected = False\n        \n        if fixed_watermark is not None:\n            fixed_match_ratio, fixed_total = fixed_watermark.verify_frozen_weights()\n            fixed_threshold = 0.95\n            fixed_detected = fixed_match_ratio > fixed_threshold\n            \n            print(f\"\\nFixed Weight Verification:\")\n            print(f\"  • Total Frozen Weights:     {fixed_total}\")\n            print(f\"  • Match Ratio:              {fixed_match_ratio:.6f}\")\n            print(f\"  • Detection Threshold:      {fixed_threshold:.6f}\")\n            print(f\"  • Fixed Weights Detected:   {'YES ✓' if fixed_detected else 'NO ✗'}\")\n            print(f\"{'-'*60}\")\n        \n        print(f\"\\nCausal Attribution Verification:\")\n        print(f\"  • Cosine Similarity:        {cosine_sim:.6f}\")\n        print(f\"  • Pearson Correlation:      {pearson_corr:.6f} (p-value: {p_value:.2e})\")\n        print(f\"  • L2 Distance:              {l2_distance:.6f}\")\n        print(f\"  • Dot Product:              {dot_product:.6f}\")\n        print(f\"  • Detection Threshold:      {causal_threshold:.6f}\")\n        print(f\"  • Causal Pattern Detected:  {'YES ✓' if causal_detected else 'NO ✗'}\")\n        print(f\"{'-'*60}\")\n        \n        overall_detected = causal_detected or fixed_detected\n        print(f\"Overall Watermark Status: {'DETECTED ✓' if overall_detected else 'NOT DETECTED ✗'}\")\n        print(f\"{'='*60}\\n\")\n        \n        verification_result = {\n            'fingerprint': fingerprint,\n            'causal_cosine_similarity': float(cosine_sim),\n            'causal_pearson_correlation': float(pearson_corr),\n            'causal_p_value': float(p_value),\n            'causal_l2_distance': float(l2_distance),\n            'causal_dot_product': float(dot_product),\n            'causal_detected': bool(causal_detected),\n            'fixed_match_ratio': float(fixed_match_ratio),\n            'fixed_total_weights': int(fixed_total),\n            'fixed_detected': bool(fixed_detected),\n            'overall_detected': bool(overall_detected)\n        }\n        \n        return verification_result\n\ndef plot_all_models_comparison(baseline_metrics, fixed_metrics, hybrid_metrics):\n    models = ['Baseline', 'Fixed Weights', 'Hybrid']\n    accuracy = [baseline_metrics['accuracy'], fixed_metrics['accuracy'], hybrid_metrics['accuracy']]\n    precision = [baseline_metrics['precision'], fixed_metrics['precision'], hybrid_metrics['precision']]\n    recall = [baseline_metrics['recall'], fixed_metrics['recall'], hybrid_metrics['recall']]\n    f1 = [baseline_metrics['f1_score'], fixed_metrics['f1_score'], hybrid_metrics['f1_score']]\n    \n    x = np.arange(len(models))\n    width = 0.2\n    \n    fig, ax = plt.subplots(figsize=(12, 6))\n    \n    bars1 = ax.bar(x - 1.5*width, accuracy, width, label='Accuracy', color='#2ecc71', edgecolor='black', alpha=0.8)\n    bars2 = ax.bar(x - 0.5*width, precision, width, label='Precision', color='#3498db', edgecolor='black', alpha=0.8)\n    bars3 = ax.bar(x + 0.5*width, recall, width, label='Recall', color='#e74c3c', edgecolor='black', alpha=0.8)\n    bars4 = ax.bar(x + 1.5*width, f1, width, label='F1-Score', color='#f39c12', edgecolor='black', alpha=0.8)\n    \n    for bars in [bars1, bars2, bars3, bars4]:\n        for bar in bars:\n            height = bar.get_height()\n            ax.text(bar.get_x() + bar.get_width()/2., height,\n                    f'{height:.3f}',\n                    ha='center', va='bottom', fontsize=9, fontweight='bold')\n    \n    ax.set_ylabel('Score', fontsize=12)\n    ax.set_title('Performance Comparison Across All Models', fontsize=14, fontweight='bold')\n    ax.set_xticks(x)\n    ax.set_xticklabels(models, fontsize=11)\n    ax.legend(fontsize=11)\n    ax.set_ylim([0, 1.1])\n    ax.grid(True, alpha=0.3, axis='y')\n    \n    plt.tight_layout()\n    plt.savefig('all_models_performance_comparison.png', dpi=300, bbox_inches='tight')\n    plt.close()\n\ndef plot_verification_comparison(baseline_ver, fixed_ver, hybrid_ver):\n    models = ['Baseline', 'Fixed\\nWeights', 'Hybrid']\n    causal_sim = [baseline_ver['causal_cosine_similarity'], \n                  fixed_ver['causal_cosine_similarity'], \n                  hybrid_ver['causal_cosine_similarity']]\n    fixed_match = [0, fixed_ver['fixed_match_ratio'], hybrid_ver['fixed_match_ratio']]\n    \n    x = np.arange(len(models))\n    width = 0.35\n    \n    fig, ax = plt.subplots(figsize=(12, 6))\n    \n    bars1 = ax.bar(x - width/2, causal_sim, width, label='Causal Similarity', \n                   color='#3498db', edgecolor='black', linewidth=1.5, alpha=0.8)\n    bars2 = ax.bar(x + width/2, fixed_match, width, label='Fixed Weight Match', \n                   color='#e74c3c', edgecolor='black', linewidth=1.5, alpha=0.8)\n    \n    for bars in [bars1, bars2]:\n        for bar in bars:\n            height = bar.get_height()\n            if height > 0:\n                ax.text(bar.get_x() + bar.get_width()/2., height,\n                        f'{height:.4f}',\n                        ha='center', va='bottom', fontsize=10, fontweight='bold')\n    \n    ax.axhline(y=0.15, color='green', linestyle='--', linewidth=2, \n               label='Causal Threshold (0.15)', alpha=0.7)\n    ax.axhline(y=0.95, color='orange', linestyle='--', linewidth=2, \n               label='Fixed Threshold (0.95)', alpha=0.7)\n    \n    ax.set_ylabel('Score', fontsize=12)\n    ax.set_title('Watermark Verification Comparison', fontsize=14, fontweight='bold')\n    ax.set_xticks(x)\n    ax.set_xticklabels(models, fontsize=11)\n    ax.legend(fontsize=10)\n    ax.set_ylim([0, 1.1])\n    ax.grid(True, alpha=0.3, axis='y')\n    \n    plt.tight_layout()\n    plt.savefig('watermark_verification_comparison.png', dpi=300, bbox_inches='tight')\n    plt.close()\n\ndef plot_performance_impact(baseline_metrics, fixed_metrics, hybrid_metrics):\n    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n    baseline_vals = [baseline_metrics['accuracy'], baseline_metrics['precision'], \n                     baseline_metrics['recall'], baseline_metrics['f1_score']]\n    fixed_drops = [(baseline_metrics['accuracy'] - fixed_metrics['accuracy']) * 100,\n                   (baseline_metrics['precision'] - fixed_metrics['precision']) * 100,\n                   (baseline_metrics['recall'] - fixed_metrics['recall']) * 100,\n                   (baseline_metrics['f1_score'] - fixed_metrics['f1_score']) * 100]\n    hybrid_drops = [(baseline_metrics['accuracy'] - hybrid_metrics['accuracy']) * 100,\n                    (baseline_metrics['precision'] - hybrid_metrics['precision']) * 100,\n                    (baseline_metrics['recall'] - hybrid_metrics['recall']) * 100,\n                    (baseline_metrics['f1_score'] - hybrid_metrics['f1_score']) * 100]\n    \n    x = np.arange(len(metrics))\n    width = 0.35\n    \n    fig, ax = plt.subplots(figsize=(12, 6))\n    \n    bars1 = ax.bar(x - width/2, fixed_drops, width, label='Fixed Weights Impact', \n                   color='#e74c3c', edgecolor='black', linewidth=1.5, alpha=0.8)\n    bars2 = ax.bar(x + width/2, hybrid_drops, width, label='Hybrid Impact', \n                   color='#f39c12', edgecolor='black', linewidth=1.5, alpha=0.8)\n    \n    for bars in [bars1, bars2]:\n        for bar in bars:\n            height = bar.get_height()\n            ax.text(bar.get_x() + bar.get_width()/2., height,\n                    f'{height:.2f}%',\n                    ha='center', va='bottom' if height >= 0 else 'top', \n                    fontsize=10, fontweight='bold')\n    \n    ax.axhline(y=0, color='black', linestyle='-', linewidth=1)\n    ax.axhline(y=1, color='green', linestyle='--', linewidth=2, \n               label='Target Max Drop (1%)', alpha=0.7)\n    \n    ax.set_ylabel('Performance Drop (%)', fontsize=12)\n    ax.set_title('Watermarking Performance Impact', fontsize=14, fontweight='bold')\n    ax.set_xticks(x)\n    ax.set_xticklabels(metrics, fontsize=11)\n    ax.legend(fontsize=11)\n    ax.grid(True, alpha=0.3, axis='y')\n    \n    plt.tight_layout()\n    plt.savefig('watermarking_performance_impact.png', dpi=300, bbox_inches='tight')\n    plt.close()\n\ndef save_results_to_file(baseline_metrics, fixed_metrics, hybrid_metrics,\n                         baseline_ver, fixed_ver, hybrid_ver):\n    results = {\n        'Baseline_Model': {\n            'Performance': {\n                'Accuracy': float(baseline_metrics['accuracy']),\n                'Precision': float(baseline_metrics['precision']),\n                'Recall': float(baseline_metrics['recall']),\n                'F1_Score': float(baseline_metrics['f1_score'])\n            },\n            'Verification': {\n                'Causal_Cosine_Similarity': float(baseline_ver['causal_cosine_similarity']),\n                'Causal_Detected': bool(baseline_ver['causal_detected']),\n                'Fixed_Detected': bool(baseline_ver['fixed_detected']),\n                'Overall_Detected': bool(baseline_ver['overall_detected'])\n            }\n        },\n        'Fixed_Weight_Model': {\n            'Performance': {\n                'Accuracy': float(fixed_metrics['accuracy']),\n                'Precision': float(fixed_metrics['precision']),\n                'Recall': float(fixed_metrics['recall']),\n                'F1_Score': float(fixed_metrics['f1_score'])\n            },\n            'Verification': {\n                'Causal_Cosine_Similarity': float(fixed_ver['causal_cosine_similarity']),\n                'Fixed_Match_Ratio': float(fixed_ver['fixed_match_ratio']),\n                'Fixed_Total_Weights': int(fixed_ver['fixed_total_weights']),\n                'Causal_Detected': bool(fixed_ver['causal_detected']),\n                'Fixed_Detected': bool(fixed_ver['fixed_detected']),\n                'Overall_Detected': bool(fixed_ver['overall_detected'])\n            }\n        },\n        'Hybrid_Model': {\n            'Performance': {\n                'Accuracy': float(hybrid_metrics['accuracy']),\n                'Precision': float(hybrid_metrics['precision']),\n                'Recall': float(hybrid_metrics['recall']),\n                'F1_Score': float(hybrid_metrics['f1_score'])\n            },\n            'Verification': {\n                'Causal_Cosine_Similarity': float(hybrid_ver['causal_cosine_similarity']),\n                'Fixed_Match_Ratio': float(hybrid_ver['fixed_match_ratio']),\n                'Fixed_Total_Weights': int(hybrid_ver['fixed_total_weights']),\n                'Causal_Detected': bool(hybrid_ver['causal_detected']),\n                'Fixed_Detected': bool(hybrid_ver['fixed_detected']),\n                'Overall_Detected': bool(hybrid_ver['overall_detected'])\n            }\n        },\n        'Performance_Impact': {\n            'Fixed_Weight_Model': {\n                'Accuracy_Drop': float(baseline_metrics['accuracy'] - fixed_metrics['accuracy']),\n                'Precision_Drop': float(baseline_metrics['precision'] - fixed_metrics['precision']),\n                'Recall_Drop': float(baseline_metrics['recall'] - fixed_metrics['recall']),\n                'F1_Score_Drop': float(baseline_metrics['f1_score'] - fixed_metrics['f1_score'])\n            },\n            'Hybrid_Model': {\n                'Accuracy_Drop': float(baseline_metrics['accuracy'] - hybrid_metrics['accuracy']),\n                'Precision_Drop': float(baseline_metrics['precision'] - hybrid_metrics['precision']),\n                'Recall_Drop': float(baseline_metrics['recall'] - hybrid_metrics['recall']),\n                'F1_Score_Drop': float(baseline_metrics['f1_score'] - hybrid_metrics['f1_score'])\n            }\n        }\n    }\n    \n    with open('cwma_experiment_results.json', 'w') as f:\n        json.dump(results, f, indent=4)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"COMPREHENSIVE EXPERIMENT RESULTS\")\n    print(\"=\"*80)\n    \n    print(\"\\n\" + \"-\"*80)\n    print(\"BASELINE MODEL (No Watermark)\")\n    print(\"-\"*80)\n    print(f\"  Accuracy:  {baseline_metrics['accuracy']:.4f}\")\n    print(f\"  Precision: {baseline_metrics['precision']:.4f}\")\n    print(f\"  Recall:    {baseline_metrics['recall']:.4f}\")\n    print(f\"  F1-Score:  {baseline_metrics['f1_score']:.4f}\")\n    print(f\"  Watermark Detected: {baseline_ver['overall_detected']}\")\n    \n    print(\"\\n\" + \"-\"*80)\n    print(\"FIXED WEIGHT WATERMARKED MODEL\")\n    print(\"-\"*80)\n    print(f\"  Accuracy:  {fixed_metrics['accuracy']:.4f}\")\n    print(f\"  Precision: {fixed_metrics['precision']:.4f}\")\n    print(f\"  Recall:    {fixed_metrics['recall']:.4f}\")\n    print(f\"  F1-Score:  {fixed_metrics['f1_score']:.4f}\")\n    print(f\"  Fixed Weight Match: {fixed_ver['fixed_match_ratio']:.4f} ({fixed_ver['fixed_total_weights']} weights)\")\n    print(f\"  Watermark Detected: {fixed_ver['overall_detected']}\")\n    print(f\"  Performance Drop: {(baseline_metrics['accuracy'] - fixed_metrics['accuracy'])*100:.2f}%\")\n    \n    print(\"\\n\" + \"-\"*80)\n    print(\"HYBRID WATERMARKED MODEL (Fixed + Causal)\")\n    print(\"-\"*80)\n    print(f\"  Accuracy:  {hybrid_metrics['accuracy']:.4f}\")\n    print(f\"  Precision: {hybrid_metrics['precision']:.4f}\")\n    print(f\"  Recall:    {hybrid_metrics['recall']:.4f}\")\n    print(f\"  F1-Score:  {hybrid_metrics['f1_score']:.4f}\")\n    print(f\"  Fixed Weight Match: {hybrid_ver['fixed_match_ratio']:.4f} ({hybrid_ver['fixed_total_weights']} weights)\")\n    print(f\"  Causal Similarity: {hybrid_ver['causal_cosine_similarity']:.4f}\")\n    print(f\"  Watermark Detected: {hybrid_ver['overall_detected']}\")\n    print(f\"  Performance Drop: {(baseline_metrics['accuracy'] - hybrid_metrics['accuracy'])*100:.2f}%\")\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"KEY FINDINGS\")\n    print(\"=\"*80)\n    print(f\"  • Baseline Model: {'NOT watermarked' if not baseline_ver['overall_detected'] else 'Watermarked (unexpected)'}\")\n    print(f\"  • Fixed Weight Model: {'Successfully watermarked' if fixed_ver['fixed_detected'] else 'Watermark not detected'}\")\n    print(f\"  • Hybrid Model: {'Successfully watermarked' if hybrid_ver['overall_detected'] else 'Watermark not detected'}\")\n    print(f\"  • Hybrid uses DUAL verification (Fixed={hybrid_ver['fixed_detected']}, Causal={hybrid_ver['causal_detected']})\")\n    print(\"=\"*80 + \"\\n\")\n\ndef main():\n    print(\"\\n\" + \"=\"*80)\n    print(\"CAUSAL WATERMARKING FOR MODEL ATTRIBUTION (CWMA)\")\n    print(\"Hybrid Approach: Fixed Weights + Gradient Perturbation\")\n    print(\"=\"*80 + \"\\n\")\n    \n    IMG_DIR = '/kaggle/input/butterfly-image-classification/train'\n    CSV_PATH = '/kaggle/input/butterfly-image-classification/Training_set.csv'\n    \n    FINGERPRINT = \"CWMA2025_HybridWatermark_SecureOwnership\"\n    \n    IMG_SIZE = 224\n    BATCH_SIZE = 32\n    NUM_EPOCHS = 15\n    LEARNING_RATE = 0.001\n    \n    print(\"Loading and preparing data...\")\n    df = pd.read_csv(CSV_PATH)\n    print(f\"Total samples: {len(df)}\")\n    print(f\"Number of classes: {df['label'].nunique()}\")\n    \n    train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df['label'], random_state=RANDOM_SEED)\n    val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=RANDOM_SEED)\n    \n    print(f\"Train samples: {len(train_df)}\")\n    print(f\"Validation samples: {len(val_df)}\")\n    print(f\"Test samples: {len(test_df)}\")\n    \n    transform_train = transforms.Compose([\n        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.RandomRotation(15),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    transform_test = transforms.Compose([\n        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    train_dataset = ButterflyDataset(train_df, IMG_DIR, transform=transform_train)\n    val_dataset = ButterflyDataset(val_df, IMG_DIR, transform=transform_test)\n    test_dataset = ButterflyDataset(test_df, IMG_DIR, transform=transform_test)\n    \n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n    \n    num_classes = len(train_dataset.labels)\n    class_names = train_dataset.labels\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"EXPERIMENT 1: BASELINE MODEL (No Watermark)\")\n    print(\"=\"*80)\n    \n    baseline_model = SimpleCNN(num_classes).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer_baseline = optim.Adam(baseline_model.parameters(), lr=LEARNING_RATE)\n    \n    baseline_history = train_model(\n        baseline_model, train_loader, val_loader, criterion, \n        optimizer_baseline, NUM_EPOCHS, watermark_type=None, \n        watermark_handler=None, model_name=\"baseline\"\n    )\n    \n    baseline_model.load_state_dict(torch.load('baseline_best.pth'))\n    \n    print(\"\\nEvaluating baseline model...\")\n    baseline_metrics = evaluate_model(baseline_model, test_loader, class_names)\n    print(baseline_metrics['classification_report'])\n    \n    plot_training_history(baseline_history, \"Baseline\")\n    plot_confusion_matrix(baseline_metrics['confusion_matrix'], class_names, \"Baseline\")\n    plot_metrics_comparison(baseline_metrics, \"Baseline\")\n    \n    print(\"\\nVerifying baseline model...\")\n    baseline_verifier = CausalWatermarkVerifier(baseline_model, num_probe_samples=100)\n    baseline_verification = baseline_verifier.verify_fingerprint(FINGERPRINT, test_loader, fixed_watermark=None)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"EXPERIMENT 2: FIXED WEIGHT WATERMARKED MODEL\")\n    print(\"=\"*80)\n    print(f\"Embedding Fingerprint: {FINGERPRINT}\")\n    print(f\"Method: Freeze 0.2% of weights based on fingerprint hash\\n\")\n    \n    fixed_model = SimpleCNN(num_classes).to(device)\n    fixed_watermark = FixedWeightWatermark(fixed_model, FINGERPRINT, freeze_ratio=0.002)\n    fixed_watermark.generate_frozen_pattern()\n    fixed_watermark.apply_frozen_weights()\n    fixed_watermark.freeze_gradient_hook()\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer_fixed = optim.Adam(fixed_model.parameters(), lr=LEARNING_RATE)\n    \n    fixed_history = train_model(\n        fixed_model, train_loader, val_loader, criterion,\n        optimizer_fixed, NUM_EPOCHS, watermark_type='fixed',\n        watermark_handler=fixed_watermark, model_name=\"fixed_weight\"\n    )\n    \n    fixed_model.load_state_dict(torch.load('fixed_weight_best.pth'))\n    \n    print(\"\\nEvaluating fixed weight model...\")\n    fixed_metrics = evaluate_model(fixed_model, test_loader, class_names)\n    print(fixed_metrics['classification_report'])\n    \n    plot_training_history(fixed_history, \"Fixed_Weight\")\n    plot_confusion_matrix(fixed_metrics['confusion_matrix'], class_names, \"Fixed_Weight\")\n    plot_metrics_comparison(fixed_metrics, \"Fixed_Weight\")\n    \n    print(\"\\nVerifying fixed weight model...\")\n    fixed_verifier = CausalWatermarkVerifier(fixed_model, num_probe_samples=100)\n    fixed_verification = fixed_verifier.verify_fingerprint(FINGERPRINT, test_loader, fixed_watermark=fixed_watermark)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"EXPERIMENT 3: HYBRID WATERMARKED MODEL (Fixed + Causal)\")\n    print(\"=\"*80)\n    print(f\"Embedding Fingerprint: {FINGERPRINT}\")\n    print(f\"Method 1: Freeze 0.2% of weights\")\n    print(f\"Method 2: Gradient perturbation for causal entanglement\\n\")\n    \n    hybrid_model = SimpleCNN(num_classes).to(device)\n    hybrid_watermark = HybridWatermark(hybrid_model, FINGERPRINT, freeze_ratio=0.002, lambda_factor=0.001)\n    hybrid_watermark.initialize()\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer_hybrid = optim.Adam(hybrid_model.parameters(), lr=LEARNING_RATE)\n    \n    hybrid_history = train_model(\n        hybrid_model, train_loader, val_loader, criterion,\n        optimizer_hybrid, NUM_EPOCHS, watermark_type='hybrid',\n        watermark_handler=hybrid_watermark, model_name=\"hybrid\"\n    )\n    \n    hybrid_model.load_state_dict(torch.load('hybrid_best.pth'))\n    \n    print(\"\\nEvaluating hybrid model...\")\n    hybrid_metrics = evaluate_model(hybrid_model, test_loader, class_names)\n    print(hybrid_metrics['classification_report'])\n    \n    plot_training_history(hybrid_history, \"Hybrid\")\n    plot_confusion_matrix(hybrid_metrics['confusion_matrix'], class_names, \"Hybrid\")\n    plot_metrics_comparison(hybrid_metrics, \"Hybrid\")\n    \n    print(\"\\nVerifying hybrid model...\")\n    hybrid_verifier = CausalWatermarkVerifier(hybrid_model, num_probe_samples=100)\n    hybrid_verification = hybrid_verifier.verify_fingerprint(FINGERPRINT, test_loader, \n                                                              fixed_watermark=hybrid_watermark.fixed_watermark)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"GENERATING COMPARISON VISUALIZATIONS\")\n    print(\"=\"*80)\n    \n    plot_all_models_comparison(baseline_metrics, fixed_metrics, hybrid_metrics)\n    plot_verification_comparison(baseline_verification, fixed_verification, hybrid_verification)\n    plot_performance_impact(baseline_metrics, fixed_metrics, hybrid_metrics)\n    \n    save_results_to_file(baseline_metrics, fixed_metrics, hybrid_metrics,\n                         baseline_verification, fixed_verification, hybrid_verification)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"EXPERIMENT COMPLETED SUCCESSFULLY\")\n    print(\"=\"*80)\n    print(\"\\nGenerated Files:\")\n    print(\"  Models:\")\n    print(\"    • baseline_best.pth\")\n    print(\"    • fixed_weight_best.pth\")\n    print(\"    • hybrid_best.pth\")\n    print(\"\\n  Individual Model Plots:\")\n    print(\"    • baseline_training_history.png\")\n    print(\"    • baseline_confusion_matrix.png\")\n    print(\"    • baseline_metrics.png\")\n    print(\"    • fixed_weight_training_history.png\")\n    print(\"    • fixed_weight_confusion_matrix.png\")\n    print(\"    • fixed_weight_metrics.png\")\n    print(\"    • hybrid_training_history.png\")\n    print(\"    • hybrid_confusion_matrix.png\")\n    print(\"    • hybrid_metrics.png\")\n    print(\"\\n  Comparison Plots:\")\n    print(\"    • all_models_performance_comparison.png\")\n    print(\"    • watermark_verification_comparison.png\")\n    print(\"    • watermarking_performance_impact.png\")\n    print(\"\\n  Results:\")\n    print(\"    • cwma_experiment_results.json\")\n    print(\"=\"*80 + \"\\n\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-22T08:12:26.656832Z","iopub.execute_input":"2025-10-22T08:12:26.657127Z","iopub.status.idle":"2025-10-22T08:23:35.489008Z","shell.execute_reply.started":"2025-10-22T08:12:26.657103Z","shell.execute_reply":"2025-10-22T08:23:35.488082Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n\n================================================================================\nCAUSAL WATERMARKING FOR MODEL ATTRIBUTION (CWMA)\nHybrid Approach: Fixed Weights + Gradient Perturbation\n================================================================================\n\nLoading and preparing data...\nTotal samples: 6499\nNumber of classes: 75\nTrain samples: 4549\nValidation samples: 975\nTest samples: 975\n\n================================================================================\nEXPERIMENT 1: BASELINE MODEL (No Watermark)\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/15 [baseline]: 100%|██████████| 143/143 [00:12<00:00, 11.19it/s, loss=4.08, acc=4.07] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss: 4.0830, Train Acc: 4.07%, Val Loss: 3.6363, Val Acc: 9.95%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/15 [baseline]: 100%|██████████| 143/143 [00:11<00:00, 12.52it/s, loss=3.45, acc=13.4]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss: 3.4547, Train Acc: 13.43%, Val Loss: 3.0936, Val Acc: 20.31%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/15 [baseline]: 100%|██████████| 143/143 [00:11<00:00, 12.57it/s, loss=2.88, acc=24.2]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss: 2.8777, Train Acc: 24.20%, Val Loss: 2.3461, Val Acc: 35.90%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/15 [baseline]: 100%|██████████| 143/143 [00:11<00:00, 12.13it/s, loss=2.58, acc=30.1]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss: 2.5802, Train Acc: 30.05%, Val Loss: 2.1728, Val Acc: 42.56%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/15 [baseline]: 100%|██████████| 143/143 [00:11<00:00, 12.40it/s, loss=2.32, acc=35.3]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss: 2.3242, Train Acc: 35.28%, Val Loss: 1.9684, Val Acc: 45.03%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/15 [baseline]: 100%|██████████| 143/143 [00:11<00:00, 12.46it/s, loss=2.12, acc=41.5]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Loss: 2.1181, Train Acc: 41.50%, Val Loss: 1.9291, Val Acc: 45.85%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/15 [baseline]: 100%|██████████| 143/143 [00:11<00:00, 12.41it/s, loss=1.95, acc=45.6]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Loss: 1.9543, Train Acc: 45.59%, Val Loss: 1.7128, Val Acc: 51.08%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/15 [baseline]: 100%|██████████| 143/143 [00:11<00:00, 12.28it/s, loss=1.79, acc=49.3]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Loss: 1.7861, Train Acc: 49.29%, Val Loss: 1.5806, Val Acc: 56.82%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/15 [baseline]: 100%|██████████| 143/143 [00:11<00:00, 12.42it/s, loss=1.66, acc=51.7]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Loss: 1.6563, Train Acc: 51.75%, Val Loss: 1.4842, Val Acc: 60.62%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/15 [baseline]: 100%|██████████| 143/143 [00:11<00:00, 12.47it/s, loss=1.51, acc=56.5] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss: 1.5084, Train Acc: 56.50%, Val Loss: 1.4002, Val Acc: 60.72%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/15 [baseline]: 100%|██████████| 143/143 [00:11<00:00, 12.28it/s, loss=1.42, acc=59.3] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Train Loss: 1.4213, Train Acc: 59.29%, Val Loss: 1.3090, Val Acc: 65.33%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/15 [baseline]: 100%|██████████| 143/143 [00:11<00:00, 12.60it/s, loss=1.36, acc=59.1] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 12: Train Loss: 1.3572, Train Acc: 59.07%, Val Loss: 1.3033, Val Acc: 65.64%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/15 [baseline]: 100%|██████████| 143/143 [00:11<00:00, 12.38it/s, loss=1.23, acc=64.7] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 13: Train Loss: 1.2307, Train Acc: 64.72%, Val Loss: 1.2598, Val Acc: 66.46%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/15 [baseline]: 100%|██████████| 143/143 [00:11<00:00, 12.39it/s, loss=1.21, acc=64.7] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 14: Train Loss: 1.2137, Train Acc: 64.65%, Val Loss: 1.2783, Val Acc: 66.97%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/15 [baseline]: 100%|██████████| 143/143 [00:11<00:00, 12.65it/s, loss=1.08, acc=67.2] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 15: Train Loss: 1.0790, Train Acc: 67.20%, Val Loss: 1.2167, Val Acc: 69.23%\n\nEvaluating baseline model...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 31/31 [00:01<00:00, 15.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"                           precision    recall  f1-score   support\n\n                   ADONIS     0.7143    0.7692    0.7407        13\nAFRICAN GIANT SWALLOWTAIL     0.7500    0.5455    0.6316        11\n           AMERICAN SNOOT     0.6154    0.7273    0.6667        11\n                    AN 88     1.0000    1.0000    1.0000        13\n                  APPOLLO     0.5000    0.6154    0.5517        13\n                    ATALA     1.0000    0.8000    0.8889        15\n BANDED ORANGE HELICONIAN     0.8333    0.3333    0.4762        15\n           BANDED PEACOCK     0.6429    0.6923    0.6667        13\n            BECKERS WHITE     0.5000    0.5000    0.5000        12\n         BLACK HAIRSTREAK     0.6364    0.5385    0.5833        13\n              BLUE MORPHO     0.5000    0.5455    0.5217        11\n        BLUE SPOTTED CROW     0.7778    0.5385    0.6364        13\n           BROWN SIPROETA     0.7059    0.8000    0.7500        15\n            CABBAGE WHITE     0.5625    0.6429    0.6000        14\n          CAIRNS BIRDWING     0.7692    0.8333    0.8000        12\n       CHECQUERED SKIPPER     0.6154    0.5714    0.5926        14\n                 CHESTNUT     0.8000    0.9231    0.8571        13\n                CLEOPATRA     0.4286    0.4286    0.4286        14\n       CLODIUS PARNASSIAN     0.5833    0.5385    0.5600        13\n          CLOUDED SULPHUR     0.2800    0.5000    0.3590        14\n        COMMON BANDED AWL     0.3333    0.6154    0.4324        13\n        COMMON WOOD-NYMPH     0.4737    0.6429    0.5455        14\n              COPPER TAIL     0.4286    0.2143    0.2857        14\n                  CRECENT     0.5385    0.5000    0.5185        14\n            CRIMSON PATCH     0.9091    0.9091    0.9091        11\n            DANAID EGGFLY     0.5714    0.2857    0.3810        14\n             EASTERN COMA     0.4375    0.5000    0.4667        14\n     EASTERN DAPPLE WHITE     0.5000    0.7143    0.5882        14\n       EASTERN PINE ELFIN     0.5000    0.3571    0.4167        14\n          ELBOWED PIERROT     0.8462    0.8462    0.8462        13\n              GOLD BANDED     0.7273    0.7273    0.7273        11\n             GREAT EGGFLY     0.6429    0.7500    0.6923        12\n                GREAT JAY     1.0000    0.2857    0.4444        14\n GREEN CELLED CATTLEHEART     0.9091    0.7692    0.8333        13\n          GREY HAIRSTREAK     0.2941    0.7692    0.4255        13\n            INDRA SWALLOW     0.7500    0.7500    0.7500        12\n          IPHICLUS SISTER     0.7857    0.7857    0.7857        14\n                    JULIA     0.4444    0.6667    0.5333        12\n             LARGE MARBLE     0.8571    0.5000    0.6316        12\n                MALACHITE     0.7273    0.7273    0.7273        11\n         MANGROVE SKIPPER     0.5000    0.6923    0.5806        13\n                   MESTRA     0.5000    0.4615    0.4800        13\n                METALMARK     0.3333    0.1818    0.2353        11\n   MILBERTS TORTOISESHELL     0.5625    0.6000    0.5806        15\n                  MONARCH     0.7692    0.7692    0.7692        13\n           MOURNING CLOAK     0.7727    0.8947    0.8293        19\n           ORANGE OAKLEAF     0.5455    0.4615    0.5000        13\n               ORANGE TIP     0.6842    0.9286    0.7879        14\n          ORCHARD SWALLOW     0.8000    0.6667    0.7273        12\n             PAINTED LADY     0.8182    0.8182    0.8182        11\n               PAPER KITE     0.7000    1.0000    0.8235        14\n                  PEACOCK     0.9000    0.7500    0.8182        12\n               PINE WHITE     0.5714    0.6154    0.5926        13\n         PIPEVINE SWALLOW     0.7273    0.6154    0.6667        13\n                 POPINJAY     0.9091    0.7692    0.8333        13\n        PURPLE HAIRSTREAK     0.5000    0.3333    0.4000        12\n          PURPLISH COPPER     0.4444    0.2857    0.3478        14\n            QUESTION MARK     0.4000    0.1667    0.2353        12\n              RED ADMIRAL     0.8333    0.7692    0.8000        13\n              RED CRACKER     0.9091    0.7143    0.8000        14\n              RED POSTMAN     0.7500    0.4286    0.5455        14\n       RED SPOTTED PURPLE     0.6875    0.8462    0.7586        13\n           SCARCE SWALLOW     0.9286    0.8667    0.8966        15\n      SILVER SPOT SKIPPER     0.8000    0.6667    0.7273        12\n            SLEEPY ORANGE     0.5556    0.3125    0.4000        16\n                SOOTYWING     0.7000    0.5385    0.6087        13\n         SOUTHERN DOGFACE     0.6667    0.6154    0.6400        13\n           STRAITED QUEEN     0.5789    0.8462    0.6875        13\n        TROPICAL LEAFWING     0.5500    0.9167    0.6875        12\n       TWO BARRED FLASHER     0.8889    0.6667    0.7619        12\n                   ULYSES     0.8571    0.9231    0.8889        13\n                  VICEROY     0.6923    0.7500    0.7200        12\n               WOOD SATYR     0.6923    0.9000    0.7826        10\n      YELLOW SWALLOW TAIL     0.5714    0.3636    0.4444        11\n          ZEBRA LONG WING     0.9091    0.9091    0.9091        11\n\n                 accuracy                         0.6410       975\n                macro avg     0.6653    0.6427    0.6378       975\n             weighted avg     0.6650    0.6410    0.6363       975\n\n\nVerifying baseline model...\n\n============================================================\nWATERMARK VERIFICATION\n============================================================\nTarget Fingerprint: CWMA2025_HybridWatermark_SecureOwnership\nProbe Samples: 100\n------------------------------------------------------------\n\nCausal Attribution Verification:\n  • Cosine Similarity:        0.003359\n  • Pearson Correlation:      0.036859 (p-value: 2.44e-01)\n  • L2 Distance:              1.411836\n  • Dot Product:              0.003359\n  • Detection Threshold:      0.150000\n  • Causal Pattern Detected:  NO ✗\n------------------------------------------------------------\nOverall Watermark Status: NOT DETECTED ✗\n============================================================\n\n\n================================================================================\nEXPERIMENT 2: FIXED WEIGHT WATERMARKED MODEL\n================================================================================\nEmbedding Fingerprint: CWMA2025_HybridWatermark_SecureOwnership\nMethod: Freeze 0.2% of weights based on fingerprint hash\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/15 [fixed_weight]: 100%|██████████| 143/143 [00:11<00:00, 12.55it/s, loss=4.27, acc=2]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss: 4.2671, Train Acc: 2.00%, Val Loss: 4.0483, Val Acc: 6.46%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/15 [fixed_weight]: 100%|██████████| 143/143 [00:11<00:00, 12.60it/s, loss=3.79, acc=7.98]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss: 3.7871, Train Acc: 7.98%, Val Loss: 3.3679, Val Acc: 15.69%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/15 [fixed_weight]: 100%|██████████| 143/143 [00:11<00:00, 12.73it/s, loss=3.19, acc=19.9]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss: 3.1903, Train Acc: 19.92%, Val Loss: 2.7397, Val Acc: 30.26%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/15 [fixed_weight]: 100%|██████████| 143/143 [00:11<00:00, 12.47it/s, loss=2.76, acc=27.9]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss: 2.7575, Train Acc: 27.94%, Val Loss: 2.4028, Val Acc: 38.05%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/15 [fixed_weight]: 100%|██████████| 143/143 [00:11<00:00, 12.25it/s, loss=2.41, acc=35.9]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss: 2.4125, Train Acc: 35.88%, Val Loss: 2.1768, Val Acc: 40.10%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/15 [fixed_weight]: 100%|██████████| 143/143 [00:11<00:00, 12.38it/s, loss=2.18, acc=39.3]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Loss: 2.1844, Train Acc: 39.35%, Val Loss: 1.9779, Val Acc: 45.64%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/15 [fixed_weight]: 100%|██████████| 143/143 [00:11<00:00, 12.22it/s, loss=2.01, acc=44.9]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Loss: 2.0076, Train Acc: 44.91%, Val Loss: 1.7467, Val Acc: 54.15%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/15 [fixed_weight]: 100%|██████████| 143/143 [00:11<00:00, 12.61it/s, loss=1.76, acc=50.1]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Loss: 1.7576, Train Acc: 50.10%, Val Loss: 1.6552, Val Acc: 56.72%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/15 [fixed_weight]: 100%|██████████| 143/143 [00:11<00:00, 12.44it/s, loss=1.64, acc=53.5]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Loss: 1.6412, Train Acc: 53.53%, Val Loss: 1.5993, Val Acc: 56.72%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/15 [fixed_weight]: 100%|██████████| 143/143 [00:11<00:00, 12.48it/s, loss=1.54, acc=56.3] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss: 1.5425, Train Acc: 56.30%, Val Loss: 1.4941, Val Acc: 57.95%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/15 [fixed_weight]: 100%|██████████| 143/143 [00:11<00:00, 12.48it/s, loss=1.46, acc=57.7]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Train Loss: 1.4568, Train Acc: 57.73%, Val Loss: 1.4461, Val Acc: 61.54%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/15 [fixed_weight]: 100%|██████████| 143/143 [00:11<00:00, 12.37it/s, loss=1.36, acc=60.9] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 12: Train Loss: 1.3596, Train Acc: 60.89%, Val Loss: 1.3791, Val Acc: 60.92%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/15 [fixed_weight]: 100%|██████████| 143/143 [00:11<00:00, 12.45it/s, loss=1.25, acc=63.9] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 13: Train Loss: 1.2474, Train Acc: 63.93%, Val Loss: 1.3712, Val Acc: 63.08%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/15 [fixed_weight]: 100%|██████████| 143/143 [00:11<00:00, 12.33it/s, loss=1.19, acc=64.5] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 14: Train Loss: 1.1938, Train Acc: 64.45%, Val Loss: 1.2718, Val Acc: 64.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/15 [fixed_weight]: 100%|██████████| 143/143 [00:11<00:00, 11.92it/s, loss=1.09, acc=67.2] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 15: Train Loss: 1.0935, Train Acc: 67.18%, Val Loss: 1.2850, Val Acc: 65.23%\n\nEvaluating fixed weight model...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 31/31 [00:01<00:00, 18.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"                           precision    recall  f1-score   support\n\n                   ADONIS     0.9167    0.8462    0.8800        13\nAFRICAN GIANT SWALLOWTAIL     0.8750    0.6364    0.7368        11\n           AMERICAN SNOOT     0.6667    0.5455    0.6000        11\n                    AN 88     0.9286    1.0000    0.9630        13\n                  APPOLLO     0.5455    0.4615    0.5000        13\n                    ATALA     0.6087    0.9333    0.7368        15\n BANDED ORANGE HELICONIAN     0.6667    0.8000    0.7273        15\n           BANDED PEACOCK     0.8182    0.6923    0.7500        13\n            BECKERS WHITE     0.2609    0.5000    0.3429        12\n         BLACK HAIRSTREAK     0.8889    0.6154    0.7273        13\n              BLUE MORPHO     0.6250    0.9091    0.7407        11\n        BLUE SPOTTED CROW     0.5294    0.6923    0.6000        13\n           BROWN SIPROETA     0.7000    0.4667    0.5600        15\n            CABBAGE WHITE     0.6923    0.6429    0.6667        14\n          CAIRNS BIRDWING     0.8000    0.6667    0.7273        12\n       CHECQUERED SKIPPER     0.5000    0.7143    0.5882        14\n                 CHESTNUT     0.7692    0.7692    0.7692        13\n                CLEOPATRA     0.4375    0.5000    0.4667        14\n       CLODIUS PARNASSIAN     0.6000    0.2308    0.3333        13\n          CLOUDED SULPHUR     0.3333    0.3571    0.3448        14\n        COMMON BANDED AWL     0.6667    0.6154    0.6400        13\n        COMMON WOOD-NYMPH     0.6250    0.3571    0.4545        14\n              COPPER TAIL     0.6000    0.4286    0.5000        14\n                  CRECENT     0.5625    0.6429    0.6000        14\n            CRIMSON PATCH     0.9167    1.0000    0.9565        11\n            DANAID EGGFLY     0.8571    0.4286    0.5714        14\n             EASTERN COMA     0.6154    0.5714    0.5926        14\n     EASTERN DAPPLE WHITE     0.4348    0.7143    0.5405        14\n       EASTERN PINE ELFIN     0.5000    0.4286    0.4615        14\n          ELBOWED PIERROT     0.7500    0.9231    0.8276        13\n              GOLD BANDED     0.8571    0.5455    0.6667        11\n             GREAT EGGFLY     0.4706    0.6667    0.5517        12\n                GREAT JAY     0.6667    0.4286    0.5217        14\n GREEN CELLED CATTLEHEART     0.9091    0.7692    0.8333        13\n          GREY HAIRSTREAK     0.6923    0.6923    0.6923        13\n            INDRA SWALLOW     0.6154    0.6667    0.6400        12\n          IPHICLUS SISTER     0.7333    0.7857    0.7586        14\n                    JULIA     0.6471    0.9167    0.7586        12\n             LARGE MARBLE     0.5000    0.2500    0.3333        12\n                MALACHITE     0.9000    0.8182    0.8571        11\n         MANGROVE SKIPPER     0.7059    0.9231    0.8000        13\n                   MESTRA     0.8750    0.5385    0.6667        13\n                METALMARK     0.0000    0.0000    0.0000        11\n   MILBERTS TORTOISESHELL     0.6667    0.5333    0.5926        15\n                  MONARCH     0.4615    0.4615    0.4615        13\n           MOURNING CLOAK     0.8000    0.8421    0.8205        19\n           ORANGE OAKLEAF     1.0000    0.4615    0.6316        13\n               ORANGE TIP     0.9286    0.9286    0.9286        14\n          ORCHARD SWALLOW     0.6875    0.9167    0.7857        12\n             PAINTED LADY     0.6667    0.5455    0.6000        11\n               PAPER KITE     0.8667    0.9286    0.8966        14\n                  PEACOCK     1.0000    0.5000    0.6667        12\n               PINE WHITE     0.7000    0.5385    0.6087        13\n         PIPEVINE SWALLOW     0.7857    0.8462    0.8148        13\n                 POPINJAY     0.8462    0.8462    0.8462        13\n        PURPLE HAIRSTREAK     0.6667    0.5000    0.5714        12\n          PURPLISH COPPER     0.5000    0.2857    0.3636        14\n            QUESTION MARK     0.5000    0.4167    0.4545        12\n              RED ADMIRAL     0.9167    0.8462    0.8800        13\n              RED CRACKER     0.6316    0.8571    0.7273        14\n              RED POSTMAN     0.9091    0.7143    0.8000        14\n       RED SPOTTED PURPLE     0.8182    0.6923    0.7500        13\n           SCARCE SWALLOW     0.7000    0.9333    0.8000        15\n      SILVER SPOT SKIPPER     0.5500    0.9167    0.6875        12\n            SLEEPY ORANGE     0.4074    0.6875    0.5116        16\n                SOOTYWING     0.4118    0.5385    0.4667        13\n         SOUTHERN DOGFACE     0.5000    0.2308    0.3158        13\n           STRAITED QUEEN     0.7500    0.6923    0.7200        13\n        TROPICAL LEAFWING     0.3889    0.5833    0.4667        12\n       TWO BARRED FLASHER     0.7000    0.5833    0.6364        12\n                   ULYSES     0.7500    0.9231    0.8276        13\n                  VICEROY     0.3704    0.8333    0.5128        12\n               WOOD SATYR     0.7500    0.6000    0.6667        10\n      YELLOW SWALLOW TAIL     0.7500    0.5455    0.6316        11\n          ZEBRA LONG WING     0.8182    0.8182    0.8182        11\n\n                 accuracy                         0.6503       975\n                macro avg     0.6728    0.6485    0.6433       975\n             weighted avg     0.6723    0.6503    0.6440       975\n\n\nVerifying fixed weight model...\n\n============================================================\nWATERMARK VERIFICATION\n============================================================\nTarget Fingerprint: CWMA2025_HybridWatermark_SecureOwnership\nProbe Samples: 100\n------------------------------------------------------------\n\nFixed Weight Verification:\n  • Total Frozen Weights:     773\n  • Match Ratio:              1.000000\n  • Detection Threshold:      0.950000\n  • Fixed Weights Detected:   YES ✓\n------------------------------------------------------------\n\nCausal Attribution Verification:\n  • Cosine Similarity:        -0.001023\n  • Pearson Correlation:      0.025494 (p-value: 4.21e-01)\n  • L2 Distance:              1.414937\n  • Dot Product:              -0.001023\n  • Detection Threshold:      0.150000\n  • Causal Pattern Detected:  NO ✗\n------------------------------------------------------------\nOverall Watermark Status: DETECTED ✓\n============================================================\n\n\n================================================================================\nEXPERIMENT 3: HYBRID WATERMARKED MODEL (Fixed + Causal)\n================================================================================\nEmbedding Fingerprint: CWMA2025_HybridWatermark_SecureOwnership\nMethod 1: Freeze 0.2% of weights\nMethod 2: Gradient perturbation for causal entanglement\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/15 [hybrid]: 100%|██████████| 143/143 [00:11<00:00, 12.31it/s, loss=4.14, acc=3.65]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss: 4.1391, Train Acc: 3.65%, Val Loss: 3.6789, Val Acc: 9.23%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/15 [hybrid]: 100%|██████████| 143/143 [00:12<00:00, 11.89it/s, loss=3.43, acc=13.6]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss: 3.4296, Train Acc: 13.59%, Val Loss: 2.8313, Val Acc: 25.03%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/15 [hybrid]: 100%|██████████| 143/143 [00:11<00:00, 12.29it/s, loss=2.83, acc=25.3]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss: 2.8317, Train Acc: 25.32%, Val Loss: 2.3664, Val Acc: 37.13%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/15 [hybrid]: 100%|██████████| 143/143 [00:12<00:00, 11.88it/s, loss=2.41, acc=34.1]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss: 2.4059, Train Acc: 34.05%, Val Loss: 2.0845, Val Acc: 42.87%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/15 [hybrid]: 100%|██████████| 143/143 [00:11<00:00, 12.28it/s, loss=2.08, acc=42]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss: 2.0810, Train Acc: 42.01%, Val Loss: 1.9233, Val Acc: 47.49%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/15 [hybrid]: 100%|██████████| 143/143 [00:11<00:00, 12.21it/s, loss=1.88, acc=47]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Loss: 1.8815, Train Acc: 46.96%, Val Loss: 1.6376, Val Acc: 55.79%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/15 [hybrid]: 100%|██████████| 143/143 [00:11<00:00, 12.24it/s, loss=1.7, acc=51.9] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Loss: 1.7043, Train Acc: 51.95%, Val Loss: 1.5580, Val Acc: 56.72%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/15 [hybrid]: 100%|██████████| 143/143 [00:11<00:00, 12.44it/s, loss=1.57, acc=55]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Loss: 1.5674, Train Acc: 55.00%, Val Loss: 1.3976, Val Acc: 60.51%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/15 [hybrid]: 100%|██████████| 143/143 [00:11<00:00, 12.39it/s, loss=1.49, acc=56.9] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Loss: 1.4877, Train Acc: 56.91%, Val Loss: 1.3666, Val Acc: 61.44%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/15 [hybrid]: 100%|██████████| 143/143 [00:11<00:00, 12.44it/s, loss=1.38, acc=60.5] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss: 1.3769, Train Acc: 60.45%, Val Loss: 1.3472, Val Acc: 62.26%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/15 [hybrid]: 100%|██████████| 143/143 [00:11<00:00, 12.27it/s, loss=1.31, acc=61.6] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Train Loss: 1.3137, Train Acc: 61.57%, Val Loss: 1.2836, Val Acc: 66.15%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/15 [hybrid]: 100%|██████████| 143/143 [00:11<00:00, 12.30it/s, loss=1.22, acc=64]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch 12: Train Loss: 1.2248, Train Acc: 64.01%, Val Loss: 1.2325, Val Acc: 67.08%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/15 [hybrid]: 100%|██████████| 143/143 [00:11<00:00, 12.48it/s, loss=1.16, acc=65.7] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 13: Train Loss: 1.1559, Train Acc: 65.71%, Val Loss: 1.1953, Val Acc: 66.87%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/15 [hybrid]: 100%|██████████| 143/143 [00:11<00:00, 12.25it/s, loss=1.09, acc=67.3] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 14: Train Loss: 1.0905, Train Acc: 67.31%, Val Loss: 1.1376, Val Acc: 69.54%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/15 [hybrid]: 100%|██████████| 143/143 [00:11<00:00, 12.34it/s, loss=1.04, acc=68.8] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 15: Train Loss: 1.0382, Train Acc: 68.83%, Val Loss: 1.1758, Val Acc: 67.79%\n\nEvaluating hybrid model...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 31/31 [00:01<00:00, 19.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"                           precision    recall  f1-score   support\n\n                   ADONIS     0.7500    0.9231    0.8276        13\nAFRICAN GIANT SWALLOWTAIL     0.7778    0.6364    0.7000        11\n           AMERICAN SNOOT     0.7500    0.8182    0.7826        11\n                    AN 88     0.9286    1.0000    0.9630        13\n                  APPOLLO     0.3438    0.8462    0.4889        13\n                    ATALA     0.9286    0.8667    0.8966        15\n BANDED ORANGE HELICONIAN     0.6111    0.7333    0.6667        15\n           BANDED PEACOCK     0.8333    0.7692    0.8000        13\n            BECKERS WHITE     0.0000    0.0000    0.0000        12\n         BLACK HAIRSTREAK     0.7692    0.7692    0.7692        13\n              BLUE MORPHO     0.6667    0.5455    0.6000        11\n        BLUE SPOTTED CROW     0.8571    0.4615    0.6000        13\n           BROWN SIPROETA     0.8000    0.8000    0.8000        15\n            CABBAGE WHITE     0.6111    0.7857    0.6875        14\n          CAIRNS BIRDWING     0.7143    0.8333    0.7692        12\n       CHECQUERED SKIPPER     0.8182    0.6429    0.7200        14\n                 CHESTNUT     0.7692    0.7692    0.7692        13\n                CLEOPATRA     0.4444    0.2857    0.3478        14\n       CLODIUS PARNASSIAN     0.0000    0.0000    0.0000        13\n          CLOUDED SULPHUR     0.5000    0.5000    0.5000        14\n        COMMON BANDED AWL     0.7273    0.6154    0.6667        13\n        COMMON WOOD-NYMPH     0.4000    0.7143    0.5128        14\n              COPPER TAIL     0.5455    0.4286    0.4800        14\n                  CRECENT     0.6250    0.7143    0.6667        14\n            CRIMSON PATCH     0.8462    1.0000    0.9167        11\n            DANAID EGGFLY     0.8333    0.3571    0.5000        14\n             EASTERN COMA     0.4286    0.6429    0.5143        14\n     EASTERN DAPPLE WHITE     0.6364    0.5000    0.5600        14\n       EASTERN PINE ELFIN     0.5333    0.5714    0.5517        14\n          ELBOWED PIERROT     0.7857    0.8462    0.8148        13\n              GOLD BANDED     0.8000    0.7273    0.7619        11\n             GREAT EGGFLY     0.5714    0.6667    0.6154        12\n                GREAT JAY     0.5333    0.5714    0.5517        14\n GREEN CELLED CATTLEHEART     0.7857    0.8462    0.8148        13\n          GREY HAIRSTREAK     0.6875    0.8462    0.7586        13\n            INDRA SWALLOW     0.7273    0.6667    0.6957        12\n          IPHICLUS SISTER     0.8000    0.8571    0.8276        14\n                    JULIA     0.6250    0.4167    0.5000        12\n             LARGE MARBLE     0.3750    0.5000    0.4286        12\n                MALACHITE     0.7500    0.5455    0.6316        11\n         MANGROVE SKIPPER     0.8462    0.8462    0.8462        13\n                   MESTRA     0.7000    0.5385    0.6087        13\n                METALMARK     0.6000    0.2727    0.3750        11\n   MILBERTS TORTOISESHELL     0.8000    0.5333    0.6400        15\n                  MONARCH     0.5882    0.7692    0.6667        13\n           MOURNING CLOAK     0.7917    1.0000    0.8837        19\n           ORANGE OAKLEAF     0.8571    0.4615    0.6000        13\n               ORANGE TIP     0.8125    0.9286    0.8667        14\n          ORCHARD SWALLOW     1.0000    0.7500    0.8571        12\n             PAINTED LADY     0.6923    0.8182    0.7500        11\n               PAPER KITE     0.6190    0.9286    0.7429        14\n                  PEACOCK     0.8182    0.7500    0.7826        12\n               PINE WHITE     0.7500    0.6923    0.7200        13\n         PIPEVINE SWALLOW     0.8462    0.8462    0.8462        13\n                 POPINJAY     0.8571    0.9231    0.8889        13\n        PURPLE HAIRSTREAK     0.5000    0.4167    0.4545        12\n          PURPLISH COPPER     0.4615    0.4286    0.4444        14\n            QUESTION MARK     1.0000    0.1667    0.2857        12\n              RED ADMIRAL     0.7059    0.9231    0.8000        13\n              RED CRACKER     0.9167    0.7857    0.8462        14\n              RED POSTMAN     0.7059    0.8571    0.7742        14\n       RED SPOTTED PURPLE     0.7692    0.7692    0.7692        13\n           SCARCE SWALLOW     0.7222    0.8667    0.7879        15\n      SILVER SPOT SKIPPER     0.5556    0.8333    0.6667        12\n            SLEEPY ORANGE     0.3750    0.5625    0.4500        16\n                SOOTYWING     0.6875    0.8462    0.7586        13\n         SOUTHERN DOGFACE     0.6667    0.6154    0.6400        13\n           STRAITED QUEEN     0.7143    0.7692    0.7407        13\n        TROPICAL LEAFWING     0.5833    0.5833    0.5833        12\n       TWO BARRED FLASHER     0.9000    0.7500    0.8182        12\n                   ULYSES     0.8571    0.9231    0.8889        13\n                  VICEROY     0.8000    0.6667    0.7273        12\n               WOOD SATYR     0.6364    0.7000    0.6667        10\n      YELLOW SWALLOW TAIL     1.0000    0.4545    0.6250        11\n          ZEBRA LONG WING     1.0000    0.8182    0.9000        11\n\n                 accuracy                         0.6810       975\n                macro avg     0.6937    0.6775    0.6688       975\n             weighted avg     0.6916    0.6810    0.6699       975\n\n\nVerifying hybrid model...\n\n============================================================\nWATERMARK VERIFICATION\n============================================================\nTarget Fingerprint: CWMA2025_HybridWatermark_SecureOwnership\nProbe Samples: 100\n------------------------------------------------------------\n\nFixed Weight Verification:\n  • Total Frozen Weights:     773\n  • Match Ratio:              1.000000\n  • Detection Threshold:      0.950000\n  • Fixed Weights Detected:   YES ✓\n------------------------------------------------------------\n\nCausal Attribution Verification:\n  • Cosine Similarity:        0.023715\n  • Pearson Correlation:      0.049671 (p-value: 1.16e-01)\n  • L2 Distance:              1.397344\n  • Dot Product:              0.023715\n  • Detection Threshold:      0.150000\n  • Causal Pattern Detected:  NO ✗\n------------------------------------------------------------\nOverall Watermark Status: DETECTED ✓\n============================================================\n\n\n================================================================================\nGENERATING COMPARISON VISUALIZATIONS\n================================================================================\n\n================================================================================\nCOMPREHENSIVE EXPERIMENT RESULTS\n================================================================================\n\n--------------------------------------------------------------------------------\nBASELINE MODEL (No Watermark)\n--------------------------------------------------------------------------------\n  Accuracy:  0.6410\n  Precision: 0.6650\n  Recall:    0.6410\n  F1-Score:  0.6363\n  Watermark Detected: False\n\n--------------------------------------------------------------------------------\nFIXED WEIGHT WATERMARKED MODEL\n--------------------------------------------------------------------------------\n  Accuracy:  0.6503\n  Precision: 0.6723\n  Recall:    0.6503\n  F1-Score:  0.6440\n  Fixed Weight Match: 1.0000 (773 weights)\n  Watermark Detected: True\n  Performance Drop: -0.92%\n\n--------------------------------------------------------------------------------\nHYBRID WATERMARKED MODEL (Fixed + Causal)\n--------------------------------------------------------------------------------\n  Accuracy:  0.6810\n  Precision: 0.6916\n  Recall:    0.6810\n  F1-Score:  0.6699\n  Fixed Weight Match: 1.0000 (773 weights)\n  Causal Similarity: 0.0237\n  Watermark Detected: True\n  Performance Drop: -4.00%\n\n================================================================================\nKEY FINDINGS\n================================================================================\n  • Baseline Model: NOT watermarked\n  • Fixed Weight Model: Successfully watermarked\n  • Hybrid Model: Successfully watermarked\n  • Hybrid uses DUAL verification (Fixed=True, Causal=False)\n================================================================================\n\n\n================================================================================\nEXPERIMENT COMPLETED SUCCESSFULLY\n================================================================================\n\nGenerated Files:\n  Models:\n    • baseline_best.pth\n    • fixed_weight_best.pth\n    • hybrid_best.pth\n\n  Individual Model Plots:\n    • baseline_training_history.png\n    • baseline_confusion_matrix.png\n    • baseline_metrics.png\n    • fixed_weight_training_history.png\n    • fixed_weight_confusion_matrix.png\n    • fixed_weight_metrics.png\n    • hybrid_training_history.png\n    • hybrid_confusion_matrix.png\n    • hybrid_metrics.png\n\n  Comparison Plots:\n    • all_models_performance_comparison.png\n    • watermark_verification_comparison.png\n    • watermarking_performance_impact.png\n\n  Results:\n    • cwma_experiment_results.json\n================================================================================\n\n","output_type":"stream"}],"execution_count":2}]}